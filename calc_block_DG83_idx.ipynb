{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate DG83 index\n",
    "Method adapted from Pinheiro et al. (2019). This uses four thresholds to apply for the blocking criterea, which are applied in turn:\n",
    "    \n",
    "    AMPLITUDE - count the number of grid squares exceeding the var threshold (which may be -ve or +ve if one looks at the anomalies or not)\n",
    "    AREA - for each blocked region count the number of grid cells at each latitude and so calculate the total area of each blocked region\n",
    "    PERSISTENCE - measure how long the block persists for, and set a threshold for at least five days\n",
    "    OVERLAP - count the number of days over which the contours for the blocked region overlap\n",
    "\n",
    "If all of these criterea are met, then blocked_day = True. Else blocked_day = False.\n",
    "    \n",
    "These thresholds are applied to two datasets: 500hPa geopotentiel height anomalies, detrended wrt surface temperature (creating a measure similar to Dole and Gordon (1983)) and the seasonal anomaly of vertically averaged potential vorticity (similar to Schweirz (2004)).  The calculation of the anomaly fields and detrending has been done in a separate notebook, and uses ``cdo`` commands: https://code.mpimet.mpg.de/projects/cdo/embedded/cdo.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import iris\n",
    "import netCDF4 as nc\n",
    "import xarray\n",
    "import numpy as np\n",
    "import scipy\n",
    "from collections import OrderedDict \n",
    "#from windspharm.iris import VectorWind\n",
    "from scipy.integrate import simps\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "#import PDF_funcs\n",
    "import cartopy.crs as ccrs\n",
    "from scipy import stats\n",
    "from itertools import groupby\n",
    "from operator import itemgetter\n",
    "#for mapping the polygon on a sphere to a polygon on a flat surface to calculate area\n",
    "import pyproj\n",
    "import math\n",
    "from shapely import geometry\n",
    "import collections\n",
    "from shapely.geometry import Point\n",
    "from matplotlib.path import Path\n",
    "import cartopy.crs as ccrs\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TM90 = xr.open_dataset(\"/rds/general/user/cmt3718/home/data/cmip6/TM90/blocked_lats/\"\n",
    "                \"TM90_UKESM1-0-LL_r19i1p1f2_EURATL_ssp585_JJA_1850-2014_corrlon.nc\")['blocking_index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_list = sorted(glob.glob(\"/rds/general/project/carl_phd/live/carl/data/cmip6/*/ssp585/day/*/zg/*r180x91*\"))[15:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_list \n",
    "dir_zg_arr = []\n",
    "for dir_list in arr_list:\n",
    "    dir_zg = ((dir_list.split(\"_\")[0]+\"_\"+dir_list.split(\"_\")[1])[:-5])\n",
    "    dir_zg_arr = np.append(dir_zg_arr, dir_zg)\n",
    "    \n",
    "unique_dir_arr=(np.unique(np.array(dir_zg_arr)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('CESM2-FV2', 'r1i1p1f1')"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_dir_arr[0].split(\"/\")[9], unique_dir_arr[0].split(\"/\")[12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/rds/general/project/carl_phd/live/carl/data/cmip6/AWI-CM-1-1-MR/ssp585/day/r1i1p1f1/tas/tas_day_AWI-CM-1-1-MR_ssp585_r1i1p1f1_18500101-19061231_r180x91.nc\n",
      "/rds/general/project/carl_phd/live/carl/data/cmip6/ACCESS-ESM1-5/ssp585/day/r1i1p1f1/tas/tas_day_ACCESS-ESM1-5_ssp585_r1i1p1f1_18500101-21001231_r180x91.nc\n",
      "/rds/general/project/carl_phd/live/carl/data/cmip6/GFDL-CM4/ssp585/day/r1i1p1f1/tas/tas_day_GFDL-CM4_historical_r1i1p1f1_gr1_18500101-18691231_r180x91.nc\n",
      "/rds/general/project/carl_phd/live/carl/data/cmip6/BCC-CSM2-MR/ssp585/day/r1i1p1f1/tas/tas_day_BCC-CSM2-MR_ssp585_r1i1p1f1_18500101-21001231_r180x91.nc\n",
      "/rds/general/project/carl_phd/live/carl/data/cmip6/BCC-CSM2-MR/ssp585/day/r1i1p1f1/tas/tas_day_BCC-CSM2-MR_ssp585_r1i1p1f1_19500101-21001231_r180x91.nc\n",
      "/rds/general/project/carl_phd/live/carl/data/cmip6/CNRM-CM6-1/ssp585/day/r3i1p1f2/tas/tas_day_CNRM-CM6-1_ssp585_r3i1p1f2_gr_20150101-21001231_r180x91.nc\n",
      "/rds/general/project/carl_phd/live/carl/data/cmip6/CNRM-CM6-1/ssp585/day/r4i1p1f2/tas/tas_day_CNRM-CM6-1_ssp585_r4i1p1f2_gr_20150101-21001231_r180x91.nc\n",
      "/rds/general/project/carl_phd/live/carl/data/cmip6/CNRM-CM6-1/ssp585/day/r1i1p1f2/tas/tas_day_CNRM-CM6-1_ssp585_r1i1p1f2_gr_20150101-21001231_r180x91.nc\n",
      "/rds/general/project/carl_phd/live/carl/data/cmip6/CNRM-CM6-1/ssp585/day/r1i1p1f2/tas/tas_day_CNRM-CM6-1_historical_r1i1p1f2_gr_18500101-20141231_r180x91.nc\n",
      "/rds/general/project/carl_phd/live/carl/data/cmip6/CNRM-CM6-1/ssp585/day/r5i1p1f2/tas/tas_day_CNRM-CM6-1_historical_r5i1p1f2_gr_18500101-20141231_r180x91.nc\n",
      "/rds/general/project/carl_phd/live/carl/data/cmip6/CNRM-CM6-1/ssp585/day/r5i1p1f2/tas/tas_day_CNRM-CM6-1_ssp585_r5i1p1f2_gr_20150101-21001231_r180x91.nc\n",
      "/rds/general/project/carl_phd/live/carl/data/cmip6/CNRM-CM6-1/ssp585/day/r2i1p1f2/tas/tas_day_CNRM-CM6-1_ssp585_r2i1p1f2_gr_20150101-21001231_r180x91.nc\n",
      "/rds/general/project/carl_phd/live/carl/data/cmip6/CNRM-CM6-1/ssp585/day/r2i1p1f2/tas/tas_day_CNRM-CM6-1_historical_r2i1p1f2_gr_18500101-20141231_r180x91.nc\n",
      "/rds/general/project/carl_phd/live/carl/data/cmip6/CNRM-CM6-1/ssp585/day/r6i1p1f2/tas/tas_day_CNRM-CM6-1_ssp585_r6i1p1f2_gr_20150101-21001231_r180x91.nc\n",
      "/rds/general/project/carl_phd/live/carl/data/cmip6/CNRM-CM6-1/ssp585/day/r6i1p1f2/tas/tas_day_CNRM-CM6-1_historical_r6i1p1f2_gr_18500101-20141231_r180x91.nc\n",
      "/rds/general/project/carl_phd/live/carl/data/cmip6/CNRM-ESM2-1/ssp585/day/r4i1p1f2/tas/tas_day_CNRM-ESM2-1_historical_r4i1p1f2_gr_18500101-20141231_r180x91.nc\n",
      "/rds/general/project/carl_phd/live/carl/data/cmip6/CNRM-ESM2-1/ssp585/day/r4i1p1f2/tas/tas_day_CNRM-ESM2-1_ssp585_r4i1p1f2_gr_20150101-21001231_r180x91.nc\n",
      "/rds/general/project/carl_phd/live/carl/data/cmip6/CNRM-ESM2-1/ssp585/day/r5i1p1f2/tas/tas_day_CNRM-ESM2-1_ssp585_r5i1p1f2_gr_20150101-21001231_r180x91.nc\n",
      "/rds/general/project/carl_phd/live/carl/data/cmip6/CNRM-ESM2-1/ssp585/day/r5i1p1f2/tas/tas_day_CNRM-ESM2-1_historical_r5i1p1f2_gr_18500101-20141231_r180x91.nc\n"
     ]
    }
   ],
   "source": [
    "arr_list=glob.glob(\"/rds/general/project/carl_phd/live/carl/data/cmip6/*/ssp585/day/*/tas/*r180x91*\")\n",
    "\n",
    "for filedir in arr_list:\n",
    "    if \"_gn_\" not in filedir:\n",
    "        print(filedir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_list=glob.glob(\"/rds/general/project/carl_phd/live/carl/data/cmip6/*/ssp585/day/*/zg/*\")\n",
    "\n",
    "dir_zg_arr = []\n",
    "for dir_list in arr_list:\n",
    "    if \"BCC-CSM2-MR\" not in arr_list:\n",
    "        dir_zg = ((dir_list.split(\"_\")[0]+\"_\"+dir_list.split(\"_\")[1])[:-5])\n",
    "        dir_zg_arr = np.append(dir_zg_arr, dir_zg)\n",
    "    \n",
    "unique_dir_arr=(np.unique(np.array(dir_zg_arr))) # use this directory to calculate LTDM\n",
    "#unique_dir_arr\n",
    "zg_files_list = []\n",
    "for i in unique_dir_arr:\n",
    "    file_arr=glob.glob(f\"{i}*r180x91.nc*\")\n",
    "    for j in file_arr:\n",
    "        if \"_gn_\" not in j:\n",
    "            if \"_gr\" not in j:\n",
    "                if \"BCC-CSM2-MR\" not in j:\n",
    "                    zg_files_list = np.append(zg_files_list, j)\n",
    "\n",
    "arr_list=glob.glob(\"/rds/general/project/carl_phd/live/carl/data/cmip6/*/ssp585/day/*/tas/*\")\n",
    "\n",
    "dir_zg_arr = []\n",
    "for dir_list in arr_list:\n",
    "    if \"BCC-CSM2-MR\" not in arr_list:\n",
    "        dir_zg = ((dir_list.split(\"_\")[0]+\"_\"+dir_list.split(\"_\")[1])[:-5])\n",
    "        dir_zg_arr = np.append(dir_zg_arr, dir_zg)\n",
    "    \n",
    "unique_dir_arr=(np.unique(np.array(dir_zg_arr))) # use this directory to calculate LTDM\n",
    "#unique_dir_arr\n",
    "tas_files_list = []\n",
    "for i in unique_dir_arr:\n",
    "    file_arr=glob.glob(f\"{i}*r180x91.nc*\")\n",
    "    for j in file_arr:\n",
    "        if \"_gn_\" not in j:\n",
    "            if \"_gr\" not in j:\n",
    "                if \"BCC-CSM2-MR\" not in j:\n",
    "                    tas_files_list = np.append(tas_files_list, j)   \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the files that are necessary  - zg_dtrnd_areaweighted_DG83_varbool is the important file, which is the end output of section1 and used in section 2 to apply the area threshold to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#implelemnt the amplitude criterea (as above but normalised for when the file is loaded again)\n",
    "#this is the file that is used from the rest of the notebook to implement the other blocking criterea\n",
    "zg_dtrnd_areaweighted_DG83_varbool=xarray.open_dataset(\"/rds/general/project/carl_phd/live/carl/data/era5/day/zg/\"\n",
    "                                            \"500zg_JJA_era5_1979-2019_daymean_EurAR5_1x1_anom_dtrnd_wrt_tas_DG83_bool.nc\")['500zg']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the variables used below; the definitions here are adapted from P19, but can be changed if desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "persis_thresh = 5 # number of days over which blocking persists\n",
    "JJA_days = 92\n",
    "# overlap_val = minimum number of cells needed to say if there is overlap between two blocked regions\n",
    "# set to 1 by default - any overlap between contours is acceptable\n",
    "#may want to change depending on spatial and temporal resolution\n",
    "overlap_val = 1\n",
    "\n",
    "amp_thresh=1e-6 # following Schweirz et al 2004, who limited spatial resolution to 100 m\n",
    "amp_thresh_anom=-1.2e-6 #threshold for a low VPV value for anomaly data\n",
    "\n",
    "amp_min_anom_thresh = 100\n",
    "\n",
    "area_thresh = 1e6 # following Pinheiro et al (2019)\n",
    "JJA_days = 92\n",
    "blocked_tiles_num_thresh = 30#24 lower threshold for area (implemented to make the area_test function marginally faster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Alternative way of defining anomalies\n",
    "\n",
    "Instead of calculating the anomaly for each day, calculate the anomaly wrt the long term daily mean. This can be derived by using the first five harmonics in the series to define the LDTM for 365 days in the year. This can then be used to subtract the data for each year. Following Pinheiro et al (2019) (and Grotjahn and Zhang (2014)), we have removed the leap years so that every year has 365 days. Note that this may cause issues with investigating blocking in DJF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First calculate the mean gph leap year for each period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cdo del29feb 2mt_1979-2019_ERA5.nc 2mt_1979-2019_ERA5_nolp.nc\n",
    "\n",
    "cdo timmean 2mt_1979-2019_ERA5_daymean.nc 2mt_1979-2019_ERA5_daymean_timmean.nc\n",
    "cdo del29feb 2mt_1979-2019_ERA5_daymean.nc  2mt_1979-2019_ERA5_daymean_nolp.nc\n",
    "cdo del29feb 2mt_1979-2019_ERA5_daymean_timmean.nc 2mt_1979-2019_ERA5_daymean_timmean_nolp.nc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mean_gph_nolp=xr.open_dataset(\"/rds/general/project/nowack_graven/live/carl/era5/2m_temperature/2mt_1979-2019_ERA5_daymean_nolp_ydayavg.nc\")['t2m']\n",
    "zg_file_nolp=xr.open_dataset(\"/rds/general/project/nowack_graven/live/carl/era5/2m_temperature/2mt_1979-2019_ERA5_daymean_nolp.nc\")['t2m']\n",
    "\n",
    "#xr.open_dataset(\"/rds/general/project/nowack_graven/live/carl/era5/2m_temperature/2mt_1979-2019_ERA5_daymean_nolp.nc\")['t2m']\n",
    "\n",
    "#mean_gph_nolp=xr.open_dataset(\"/rds/general/project/nowack_graven/live/carl/era5/2m_temperature/2mt_1979-2019_ERA5_daymean_timmean_nolp.nc\")['t2m']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "mean_gph_nolp=xr.open_dataset(\"/rds/general/project/carl_phd/live/carl/data/cmip6/BCC-CSM2-MR/ssp585/day/r1i1p1f1/zg/\"\n",
    "\"500zg_day_BCC-CSM2-MR_ssp585_r1i1p1f1_19500101-21001231_r180x91.nc\")['zg'].squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.DataArray 'zg' (time: 55115, lat: 91, lon: 180)>\n",
       "[902783700 values with dtype=float32]\n",
       "Coordinates:\n",
       "  * lon      (lon) float64 0.0 2.0 4.0 6.0 8.0 ... 350.0 352.0 354.0 356.0 358.0\n",
       "  * lat      (lat) float64 -90.0 -88.0 -86.0 -84.0 -82.0 ... 84.0 86.0 88.0 90.0\n",
       "    plev     float64 5e+04\n",
       "  * time     (time) object 1950-01-01 12:00:00 ... 2100-12-31 12:00:00\n",
       "Attributes:\n",
       "    standard_name:  geopotential_height\n",
       "    long_name:      Geopotential Height\n",
       "    units:          m\n",
       "    comment:        Geopotential is the sum of the specific gravitational pot...\n",
       "    original_name:  Z3\n",
       "    cell_methods:   time: mean (interval: 5 minutes)\n",
       "    cell_measures:  area: areacella"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_gph_nolp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55115, 91, 180)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LTDM_arr = np.zeros(mean_gph_nolp.shape)\n",
    "LTDM_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(365, 91, 180)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_gph_nolp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_gph_nolp=xr.open_dataset(\"/rds/general/project/carl_phd/live/carl/data/cmip6/BCC-CSM2-MR/ssp585/day/r1i1p1f1/zg/\"\n",
    "\"500zg_day_BCC-CSM2-MR_ssp585_r1i1p1f1_19500101-21001231_r180x91_ydayavg.nc\")['zg'].squeeze()\n",
    "\n",
    "def calc_LTDM(test_lat, test_lon):\n",
    "    #calcualte the discrete Fourier transform for each of the 365 days\n",
    "    y = mean_gph_nolp[:,test_lat, test_lon]\n",
    "    rft = np.fft.fft(y)\n",
    "    #only the first 6 harmonics are selected\n",
    "    rft[6:] = 0   # set the subsequent harmonics to zero\n",
    "    LTDM = np.fft.ifft(rft)\n",
    "    return np.real(LTDM)\n",
    "\n",
    "LTDM_arr = np.zeros(mean_gph_nolp.shape)\n",
    "for test_lat in range(mean_gph_nolp.shape[1]):\n",
    "    for test_lon in range(mean_gph_nolp.shape[2]):\n",
    "        LTDM_arr[:,test_lat,test_lon]=calc_LTDM(test_lat, test_lon)\n",
    "#LTDM_arr_xr = xarray.DataArray(LTDM_arr, coords=(mean_gph_nolp['time'][:365],mean_gph_nolp['lat'],mean_gph_nolp['lon']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "LTDM_arr_xr = xarray.DataArray(LTDM_arr, coords=(mean_gph_nolp['time'][:365],mean_gph_nolp['lat'],mean_gph_nolp['lon']))\n",
    "LTDM_arr_xr.to_netcdf(\"/rds/general/project/carl_phd/live/carl/data/cmip6/BCC-CSM2-MR/ssp585/day/r1i1p1f1/zg/\"\n",
    "                      \"500zg_day_BCC-CSM2-MR_ssp585_r1i1p1f1_19500101-21001231_r180x91_ydayavg.nc\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "LTDM_arr_xr = xarray.DataArray(LTDM_arr, coords=(mean_gph_nolp['time'][:365],mean_gph_nolp['lat'],mean_gph_nolp['lon']))\n",
    "LTDM_arr_xr.to_netcdf(\"/rds/general/project/nowack_graven/live/carl/era5/2m_temperature/2mt_1x1_1979-2019_daymean_LTDM_nolp.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "LTDM_arr_xr.to_netcdf(f\"/rds/general/project/carl_phd/live/carl/data/cmip6/BCC-CSM2-MR/ssp585/day/r1i1p1f1/zg/\"\n",
    "                      \"500zg_day_BCC-CSM2-MR_ssp585_r1i1p1f1_19500101-21001231_r180x91_LTDM.nc\")\n",
    "#zg_file_no_lp_anom = [(zg_file_no_lp[i*365:(i+1)*365,:,:]-LTDM_arr_xr.values) for i in range(41)] \n",
    "#LTDM_arr_tot = np.concatenate(LTDM_arr, axis = 0)\n",
    "#zg_file_no_lp_anom_xr = xarray.concat(zg_file_no_lp_anom, 'time')\n",
    "#zg_file_no_lp_anom_xr.to_netcdf(\"/rds/general/project/carl_phd/live/carl/data/era5/day/zg/tot/500zg_1x1_1979-2019_daymean_LTDManom_nolp.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LTDM_arr_xr.to_netcdf(f\"/rds/general/project/carl_phd/live/carl/data/era5/day/zg/tot/500zg_1979-2019_daymean_LTDM.nc\")\n",
    "zg_file_nolp_anom = [(zg_file_nolp[i*365:(i+1)*365,:,:]-LTDM_arr_xr.values) for i in range(41)] \n",
    "#LTDM_arr_tot = np.concatenate(LTDM_arr, axis = 0)\n",
    "#zg_file_no_lp_anom_xr = xarray.concat(zg_file_no_lp_anom, 'time')\n",
    "#zg_file_no_lp_anom_xr.to_netcdf(\"/rds/general/project/nowack_graven/live/carl/era5/2m_temperature/2mt_1x1_1979-2019_daymean_LTDM_nolp_anom.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cmt3718/anaconda3/envs/odin/lib/python3.6/site-packages/xarray/coding/times.py:255: FutureWarning: the 'box' keyword is deprecated and will be removed in a future version. Please take steps to stop the use of 'box'\n",
      "  unique_timedeltas = pd.to_timedelta(unique_timedeltas, box=False)\n"
     ]
    }
   ],
   "source": [
    "zg_file_nolp_anom_xr = xarray.concat(zg_file_nolp_anom, 'time')\n",
    "zg_file_nolp_anom_xr.to_netcdf(\"/rds/general/project/nowack_graven/live/carl/era5/2m_temperature/2mt_1x1_1979-2019_daymean_LTDM_nolp_anom.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'latitude'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/home/cmt3718/anaconda3/envs/odin/lib/python3.6/site-packages/xarray/core/dataarray.py\u001b[0m in \u001b[0;36m_getitem_coord\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    465\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m             \u001b[0mvar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_coords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'latitude'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-f9e6dcb80c0f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mLTDM_arr_xr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxarray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataArray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLTDM_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoords\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean_gph_nolp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m365\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmean_gph_nolp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'latitude'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmean_gph_nolp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'longitude'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/cmt3718/anaconda3/envs/odin/lib/python3.6/site-packages/xarray/core/dataarray.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    474\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 476\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_coord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    477\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    478\u001b[0m             \u001b[0;31m# xarray-style array indexing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/cmt3718/anaconda3/envs/odin/lib/python3.6/site-packages/xarray/core/dataarray.py\u001b[0m in \u001b[0;36m_getitem_coord\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    468\u001b[0m             \u001b[0mdim_sizes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m             _, key, var = _get_virtual_variable(\n\u001b[0;32m--> 470\u001b[0;31m                 self._coords, key, self._level_coords, dim_sizes)\n\u001b[0m\u001b[1;32m    471\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_replace_maybe_drop_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/cmt3718/anaconda3/envs/odin/lib/python3.6/site-packages/xarray/core/dataset.py\u001b[0m in \u001b[0;36m_get_virtual_variable\u001b[0;34m(variables, key, level_vars, dim_sizes)\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0mref_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdim_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_index_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_level_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mref_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         \u001b[0mref_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mref_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mvar_name\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'latitude'"
     ]
    }
   ],
   "source": [
    "LTDM_arr_xr = xarray.DataArray(LTDM_arr, coords=(mean_gph_nolp['time'][:365],mean_gph_nolp['latitude'],mean_gph_nolp['longitude']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the .groupby('time.dayofyear') groups by the ordinal day\n",
    "#which is an integer that specifies the number of days since 31st December the previous year\n",
    "#as such, the value of it differs for leap years, so to calculate the group mean\n",
    "#a new modified variable needs to be created\n",
    "#this has been adapted from https://github.com/pydata/xarray/issues/1844\n",
    "zg_file=xarray.open_dataset(\"/rds/general/project/carl_phd/live/carl/data/era5/day/zg/tot/500zg_1x1_1979-2019_daymean.nc\")['z']/9.80665\n",
    "dates = zg_file['time']\n",
    "\n",
    "da = zg_file['time']\n",
    "\n",
    "not_leap_year = xarray.DataArray(~da.indexes['time'].is_leap_year, coords=da.coords)\n",
    "\n",
    "march_or_later = da.time.dt.month >= 3\n",
    "\n",
    "ordinal_day = da.time.dt.dayofyear\n",
    "\n",
    "modified_ordinal_day = ordinal_day + (not_leap_year & march_or_later)\n",
    "\n",
    "modified_ordinal_day = modified_ordinal_day.rename('modified_ordinal_day')\n",
    "\n",
    "mean_gph = zg_file.groupby(modified_ordinal_day).mean('time')\n",
    "#to remove the leap year\n",
    "mean_gph_nolp = xarray.concat([mean_gph[:59,:,:], mean_gph[60:,:,:]], 'modified_ordinal_day')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Derive the long term daily mean for each value in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Calculate and save the amplitude critereon\n",
    "\n",
    "This produces the zg_dtrnd_areaweighted_DG83_varbool file which is loaded above.\n",
    "\n",
    "Note the ``calc_DG83_area_weighted_bool.py`` and ``detrend_zg_wrt_tas.py`` scripts are also used to apply the transformation of eq. (3) from P19 and to linearly detrend the data with respect to the surface temperature, to remove the local mean.\n",
    "\n",
    "the anomaly file is calculated using the command: \n",
    "``cdo -ydaysub infile -ydayavg infile outfile``\n",
    "cdo vertmean fldmean and timstd are used to calculate the vertical mean, field (spatial) mean and the standard deviation over time.\n",
    "\n",
    "Following P19, there is a varying anomaly threshold (equation 5) that is applied to both datasets, which is then used to calculate whether or not each grid cell exceeds its local theshold.\n",
    "\n",
    "Note: this doesn't need to be run since these files have been saved below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.DataArray 'z' (time: 3772, latitude: 56, longitude: 61)>\n",
       "[12885152 values with dtype=float64]\n",
       "Coordinates:\n",
       "  * longitude  (longitude) float32 -15.0 -14.0 -13.0 -12.0 ... 43.0 44.0 45.0\n",
       "  * latitude   (latitude) float32 80.0 79.0 78.0 77.0 ... 28.0 27.0 26.0 25.0\n",
       "  * time       (time) datetime64[ns] 1979-06-01T10:30:00 ... 2019-08-31T10:30:00"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LTDM_anom_nolp=xr.open_dataset(\"/rds/general/project/carl_phd/live/carl/data/era5/day/zg/LTDM/500zg_1x1_1979-2019_daymean_LTDManom_nolp.nc\")['z']\n",
    "\n",
    "\n",
    "xr.open_dataset(\"/rds/general/project/carl_phd/live/carl/data/era5/day/zg/LTDM/500zg_JJA_1x1_1979-2019_daymean_LTDManom_EurAR5plus5.nc\")['z']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.DataArray 'z' (time: 3772, latitude: 46, longitude: 51)>\n",
       "[8849112 values with dtype=float64]\n",
       "Coordinates:\n",
       "  * longitude  (longitude) float32 -10.0 -9.0 -8.0 -7.0 ... 37.0 38.0 39.0 40.0\n",
       "  * latitude   (latitude) float32 75.0 74.0 73.0 72.0 ... 33.0 32.0 31.0 30.0\n",
       "  * time       (time) datetime64[ns] 1979-06-01T10:30:00 ... 2019-08-31T10:30:00"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xr.open_dataset(\"/rds/general/project/carl_phd/live/carl/data/era5/day/zg/LTDM/500zg_JJA_1x1_1979-2019_daymean_LTDManom_EurAR5.nc\")['z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"/rds/general/project/nowack_graven/live/carl/era5/2m_temperature/*glo.nc*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.DataArray 't2m' (time: 2920, latitude: 181, longitude: 360)>\n",
       "[190267200 values with dtype=float32]\n",
       "Coordinates:\n",
       "  * longitude  (longitude) float32 0.0 1.0 2.0 3.0 ... 356.0 357.0 358.0 359.0\n",
       "  * latitude   (latitude) float32 90.0 89.0 88.0 87.0 ... -88.0 -89.0 -90.0\n",
       "  * time       (time) datetime64[ns] 1979-01-01 ... 1979-12-31T21:00:00\n",
       "Attributes:\n",
       "    units:      K\n",
       "    long_name:  2 metre temperature"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xr.open_dataset(\"/rds/general/project/nowack_graven/live/carl/era5/2m_temperature/ERA5_1979_2m_temperature_glo.nc\")['t2m']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.Dataset>\n",
       "Dimensions:                        (latitude: 46, longitude: 51, time: 365)\n",
       "Coordinates:\n",
       "  * time                           (time) datetime64[ns] 1979-01-01T10:30:00 ... 1979-12-31T10:30:00\n",
       "  * latitude                       (latitude) float32 75.0 74.0 ... 31.0 30.0\n",
       "  * longitude                      (longitude) float32 -10.0 -9.0 ... 39.0 40.0\n",
       "Data variables:\n",
       "    __xarray_dataarray_variable__  (time, latitude, longitude) float64 ..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xr.open_dataset(\"/rds/general/project/carl_phd/live/carl/data/era5/day/2t/2mt_1979-2019_1x1_EurAR5_daymean_LTDM.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.Dataset>\n",
       "Dimensions:    (latitude: 181, longitude: 360, time: 3772)\n",
       "Coordinates:\n",
       "  * longitude  (longitude) float32 0.0 1.0 2.0 3.0 ... 356.0 357.0 358.0 359.0\n",
       "  * latitude   (latitude) float32 90.0 89.0 88.0 87.0 ... -88.0 -89.0 -90.0\n",
       "  * time       (time) datetime64[ns] 1979-06-01T10:30:00 ... 2019-08-31T10:30:00\n",
       "Data variables:\n",
       "    z          (time, latitude, longitude) float64 ...\n",
       "Attributes:\n",
       "    CDI:          Climate Data Interface version 1.9.0 (http://mpimet.mpg.de/...\n",
       "    Conventions:  CF-1.6\n",
       "    history:      Tue Jan 28 09:55:50 2020: cdo select,season=JJA /rds/genera...\n",
       "    CDO:          Climate Data Operators version 1.9.0 (http://mpimet.mpg.de/..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xr.open_dataset(\"/rds/general/project/carl_phd/live/carl/data/era5/day/zg/LTDM/500zg_JJA_1x1_1979-2019_daymean_LTDManom.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdo sellonlatbox,-15,45,25,80 2mt_1x1_1979-2019_daymean_LTDM_nolp_anom.nc 2mt_1x1_1979-2019_daymean_LTDM_nolp_anom_Eurplus5.nc\n",
    "cdo fldmean 2mt_1x1_1979-2019_daymean_LTDM_nolp_anom_Eurplus5.nc 2mt_1x1_1979-2019_daymean_LTDM_nolp_anom_Eurplus5_fldmean.nc\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.Dataset>\n",
       "Dimensions:    (latitude: 56, longitude: 61, time: 14965)\n",
       "Coordinates:\n",
       "  * longitude  (longitude) float32 -15.0 -14.0 -13.0 -12.0 ... 43.0 44.0 45.0\n",
       "  * latitude   (latitude) float32 80.0 79.0 78.0 77.0 ... 28.0 27.0 26.0 25.0\n",
       "  * time       (time) object 1979-01-01 10:30:00 ... 2019-12-31 10:30:00\n",
       "Data variables:\n",
       "    t2m        (time, latitude, longitude) float64 ...\n",
       "Attributes:\n",
       "    CDI:          Climate Data Interface version 1.9.0 (http://mpimet.mpg.de/...\n",
       "    Conventions:  CF-1.6\n",
       "    history:      Thu Mar 05 12:16:48 2020: cdo sellonlatbox,-15,45,25,80 2mt...\n",
       "    CDO:          Climate Data Operators version 1.9.0 (http://mpimet.mpg.de/..."
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xarray.open_dataset(\"/rds/general/project/carl_phd/live/carl/data/era5/day/2t/\"\n",
    "                    \"2mt_1x1_1979-2019_daymean_LTDM_nolp_anom_Eurplus5.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load zg and tas files\n",
    "zg_file=xarray.open_dataset(\"/rds/general/project/carl_phd/live/carl/data/era5/day/zg/500zg_JJA_era5_1979-2019_3hr_1x1_daymean_anom_EurAR5.nc\")['z']\n",
    "tas_file=xarray.open_dataset(\"/rds/general/project/carl_phd/live/carl/data/era5/day/2t/2t_JJA_era5_1979-2019_3hr_1x1_daymean_anom_EurAR5.nc\")['t2m']\n",
    "#for the LTDM anom method\n",
    "#zg_anom=xarray.open_dataset(\"/rds/general/project/carl_phd/live/carl/data/era5/day/zg/tot/500zg_JJA_1x1_1979-2019_daymean_LTDManom_EurAR5.nc\")['z']\n",
    "#tas_file=xarray.open_dataset(\"/rds/general/project/carl_phd/live/carl/data/era5/day/2t/2t_JJA_era5_1979-2019_3hr_1x1_daymean_anom_EurAR5.nc\")['t2m']\n",
    "\n",
    "#for the LTDM anom method\n",
    "zg_anom=xarray.open_dataset(\"/rds/general/project/carl_phd/live/carl/data/era5/day/zg/LTDM/500zg_JJA_1x1_1979-2019_daymean_LTDManom_Eurplus5.nc\")['z']\n",
    "tas_file=xarray.open_dataset(\"/rds/general/project/carl_phd/live/carl/data/era5/day/2t/2mt_1x1_1979-2019_daymean_LTDM_nolp_anom_Eurplus5.nc\")['t2m']\n",
    "\n",
    "zg_anom_fldmean=xarray.open_dataset(\"/rds/general/project/carl_phd/live/carl/data/era5/day/zg/LTDM/\"\n",
    "                                    \"500zg_JJA_1x1_1979-2019_daymean_LTDManom_Eurplus5_fldmean.nc\")['z'].squeeze()\n",
    "tas_anom_fldmean=xarray.open_dataset(\"/rds/general/project/carl_phd/live/carl/data/era5/day/2t/\"\n",
    "                                     \"2mt_JJA_1x1_1979-2019_daymean_LTDM_nolp_anom_Eurplus5_fldmean.nc\")['t2m'].squeeze()\n",
    "\n",
    "\n",
    "#500zg_JJA_1x1_1979-2019_daymean_LTDManom_EurAR5_fldmean.nc\n",
    "\n",
    "#cdo -b 32 sellonlatbox,-10,40,30,75 -ydaysub 2t_JJA_era5_1979-2019_3hr_1x1_daymean.nc\n",
    "#-ydayavg 2t_JJA_era5_1979-2019_3hr_1x1_daymean.nc 2t_JJA_era5_1979-2019_3hr_1x1_daymean_anom_EurAR5.nc\n",
    "\n",
    "\n",
    "#load weighted anomaly means to calculate the tas/zg trend\n",
    "#tas_anom_fldmean = xarray.open_dataset(\"/rds/general/project/carl_phd/live/carl/data/era5/day/2t/2t_JJA_era5_1979-2019_3hr_1x1_daymean_EurAR5_fldmean_anom.nc\")['t2m'][:,0,0]\n",
    "#zg_anom_fldmean = xarray.open_dataset(\"/rds/general/project/carl_phd/live/carl/data/era5/day/zg/tot/500zg_JJA_1x1_1979-2019_daymean_LTDManom_EurAR5_fldmean.nc\")['z'][:,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<xarray.DataArray 't2m' (time: 3772)>\n",
       " array([3.146512, 3.192198, 3.018844, ..., 5.440489, 5.239959, 5.086525])\n",
       " Coordinates:\n",
       "     lon      float32 0.0\n",
       "     lat      float32 0.0\n",
       "   * time     (time) object 1979-06-01 10:30:00 ... 2019-08-31 10:30:00,\n",
       " <xarray.DataArray 'z' (time: 3772)>\n",
       " array([ 92.872425, 107.02846 , 101.841373, ..., 116.81977 , 113.61184 ,\n",
       "         92.320637])\n",
       " Coordinates:\n",
       "     lon      float32 0.0\n",
       "     lat      float32 0.0\n",
       "   * time     (time) datetime64[ns] 1979-06-01T10:30:00 ... 2019-08-31T10:30:00)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tas_anom_fldmean, zg_anom_fldmean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "time, lat, lon = zg_anom['time'], zg_anom['latitude'], zg_anom['longitude']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to detrend the data with respect to the global mean surface temperature\n",
    "def dtrnd_wrt_tas(anom_var, tas_file, var = \"zg\"):\n",
    "    if var == \"psl\":\n",
    "        lat_len = anom_var.shape[1]\n",
    "        lon_len = anom_var.shape[2]\n",
    "    if var == \"zg\":    \n",
    "        lat_len = anom_var.shape[2]\n",
    "        lon_len = anom_var.shape[3]        \n",
    "    dtrnd_tas_arr = np.zeros((anom_var.shape[0],lat_len,lon_len))\n",
    "    x = np.array(tas_file.variables['tas'][cutoff:,0,0]) - 273.15\n",
    "    for i in range(lat_len):\n",
    "        for j in range(lon_len):\n",
    "            if var == \"zg\":\n",
    "                y = zg_file['zg'][cutoff:,plev,i,j]\n",
    "            if var == \"psl\":\n",
    "                y = psl_file['psl'][cutoff:,i,j]/100 \n",
    "            coeffs_glo_mean_tas = np.polyfit(x, y, 1)\n",
    "            m_arr = np.arange(len(y))\n",
    "            dtrnd_tas_arr[:,i,j] = y - coeffs_glo_mean_tas[0]*x - coeffs_glo_mean_tas[1]\n",
    "    return dtrnd_tas_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<xarray.DataArray 't2m' (time: 14965)>\n",
       " array([-6.90991 , -7.247134, -7.578977, ..., -2.585275, -2.84168 , -3.125893])\n",
       " Coordinates:\n",
       "     lon      float32 0.0\n",
       "     lat      float32 0.0\n",
       "   * time     (time) object 1979-01-01 10:30:00 ... 2019-12-31 10:30:00,\n",
       " <xarray.DataArray 'z' (time: 3772)>\n",
       " array([ 92.872425, 107.02846 , 101.841373, ..., 116.81977 , 113.61184 ,\n",
       "         92.320637])\n",
       " Coordinates:\n",
       "     lon      float32 0.0\n",
       "     lat      float32 0.0\n",
       "   * time     (time) datetime64[ns] 1979-06-01T10:30:00 ... 2019-08-31T10:30:00)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate trend\n",
    "x=tas_anom_fldmean\n",
    "y=zg_anom_fldmean\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(x, y)\n",
    "#detrend data - takes a few seconds\n",
    "zg_dtrnd = [zg - tas_anom_fldmean[i]*slope - intercept for i, zg in enumerate((zg_anom[:]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatenate the set of xarrays created above - also takes a few seconds\n",
    "zg_dtrnd_concat = xarray.concat(zg_dtrnd, \"time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "zg_anom_dtrnd_xr = xarray.open_dataset(\"/rds/general/project/carl_phd/live/carl/data/era5/day/zg/\"\n",
    "                                       \"500zg_JJA_era5_1979-2019_daymean_EurAR5_1x1_anom_dtrnd_wrt_tas.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.DataArray (time: 3772, latitude: 56, longitude: 61)>\n",
       "array([[[-87.500906, -83.967214, ...,  21.394114,  21.067454],\n",
       "        [-76.414968, -72.228445, ...,  28.33845 ,  27.50593 ],\n",
       "        ...,\n",
       "        [-60.463796, -56.375417, ..., -25.16448 , -23.455007],\n",
       "        [-56.993093, -53.502371, ..., -24.078054, -23.029226]],\n",
       "\n",
       "       [[-66.068383, -62.895531, ...,  61.955543,  61.050758],\n",
       "        [-68.652856, -65.463402, ...,  77.568824,  76.215797],\n",
       "        ...,\n",
       "        [-48.255883, -40.976586, ..., -34.26809 , -29.914086],\n",
       "        [-41.429223, -35.905297, ..., -33.40725 , -29.709496]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 96.843815,  96.997135, ..., 197.304264, 197.163639],\n",
       "        [ 77.83112 ,  77.758366, ..., 208.259342, 208.703678],\n",
       "        ...,\n",
       "        [-25.06097 , -24.711361, ..., -44.941341, -46.220638],\n",
       "        [-30.025326, -28.977474, ..., -53.822201, -55.048763]],\n",
       "\n",
       "       [[152.214991, 150.538722, ..., 182.146632, 183.220362],\n",
       "        [126.363917, 124.076808, ..., 184.911769, 186.551417],\n",
       "        ...,\n",
       "        [-28.754247, -28.944677, ..., -50.118505, -50.981786],\n",
       "        [-33.667821, -34.067724, ..., -58.418309, -59.105809]]])\n",
       "Coordinates:\n",
       "  * longitude  (longitude) float32 -15.0 -14.0 -13.0 -12.0 ... 43.0 44.0 45.0\n",
       "  * latitude   (latitude) float32 80.0 79.0 78.0 77.0 ... 28.0 27.0 26.0 25.0\n",
       "    lon        float32 0.0\n",
       "    lat        float32 0.0\n",
       "Dimensions without coordinates: time"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zg_dtrnd_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "zg_dtrnd_concat.to_netcdf(\"/rds/general/project/carl_phd/live/carl/data/era5/day/zg/LTDM/500zg_JJA_era5_1979-2019_daymean_Eurplus5_1x1_LTDManom_dtrnd_wrt_tas.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the attrs for the new xarray\n",
    "zg_attrs = OrderedDict()\n",
    "zg_attrs['units'] = 'm'\n",
    "zg_attrs['long_name'] = 'Geopotential height'\n",
    "zg_attrs['level'] = '500 hPa'\n",
    "zg_attrs['dataset'] = 'era5'\n",
    "zg_attrs['detrended'] = '500 hPa vs surface temperature Europe fldmeam anom'\n",
    "zg_attrs['date_range'] = 'JJA 1979-2019'\n",
    "#transfer the concatenated xarray into a DataArray to redefine the axes and save\n",
    "zg_anom_dtrnd_xr = xarray.DataArray(zg_dtrnd_concat, coords=(zg_anom['time'],zg_anom['latitude'],zg_anom['longitude']), name = \"500zg\", attrs=zg_attrs)\n",
    "#save here because this file is also useful for the SOM plotting\n",
    "zg_anom_dtrnd_xr.to_netcdf(\"/rds/general/project/carl_phd/live/carl/data/era5/day/zg/LTDM/\"\n",
    "                           \"500zg_JJA_era5_1979-2019_daymean_Eurplus5_1x1_LTDManom_dtrnd_wrt_tas.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.DataArray '500zg' (time: 3772, latitude: 56, longitude: 61)>\n",
       "array([[[-87.500906, -83.967214, ...,  21.394114,  21.067454],\n",
       "        [-76.414968, -72.228445, ...,  28.33845 ,  27.50593 ],\n",
       "        ...,\n",
       "        [-60.463796, -56.375417, ..., -25.16448 , -23.455007],\n",
       "        [-56.993093, -53.502371, ..., -24.078054, -23.029226]],\n",
       "\n",
       "       [[-66.068383, -62.895531, ...,  61.955543,  61.050758],\n",
       "        [-68.652856, -65.463402, ...,  77.568824,  76.215797],\n",
       "        ...,\n",
       "        [-48.255883, -40.976586, ..., -34.26809 , -29.914086],\n",
       "        [-41.429223, -35.905297, ..., -33.40725 , -29.709496]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 96.843815,  96.997135, ..., 197.304264, 197.163639],\n",
       "        [ 77.83112 ,  77.758366, ..., 208.259342, 208.703678],\n",
       "        ...,\n",
       "        [-25.06097 , -24.711361, ..., -44.941341, -46.220638],\n",
       "        [-30.025326, -28.977474, ..., -53.822201, -55.048763]],\n",
       "\n",
       "       [[152.214991, 150.538722, ..., 182.146632, 183.220362],\n",
       "        [126.363917, 124.076808, ..., 184.911769, 186.551417],\n",
       "        ...,\n",
       "        [-28.754247, -28.944677, ..., -50.118505, -50.981786],\n",
       "        [-33.667821, -34.067724, ..., -58.418309, -59.105809]]])\n",
       "Coordinates:\n",
       "  * time       (time) datetime64[ns] 1979-06-01T10:30:00 ... 2019-08-31T10:30:00\n",
       "  * latitude   (latitude) float32 80.0 79.0 78.0 77.0 ... 28.0 27.0 26.0 25.0\n",
       "  * longitude  (longitude) float32 -15.0 -14.0 -13.0 -12.0 ... 43.0 44.0 45.0\n",
       "Attributes:\n",
       "    units:       m\n",
       "    long_name:   Geopotential height\n",
       "    level:       500 hPa\n",
       "    dataset:     era5\n",
       "    detrended:   500 hPa vs surface temperature Europe fldmeam anom\n",
       "    date_range:  JJA 1979-2019"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zg_anom_dtrnd_xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code for applying the latitudinal coefficient\n",
    "#as per Dole and Gordon (1983), need to normalize the detrended 500hPa geopotential height data by a latitutdinal coefficient (eq1 from DG 1983)\n",
    "lat_coeff = np.sin(np.radians(45))\n",
    "#use xarrays in list comprehension and then concatenante thm later to form the list\n",
    "zg_dtrnd_norm = [lat_coeff*(zg_anom_dtrnd_xr[:,i,:])/np.sin(np.radians(lat)) for i, lat in enumerate(np.array(lat))]\n",
    "\n",
    "#define the attrs for the new xarray\n",
    "zg_attrs = OrderedDict()\n",
    "zg_attrs['units'] = 'm'\n",
    "zg_attrs['long_name'] = 'Geopotential height'\n",
    "zg_attrs['level'] = '500 hPa'\n",
    "zg_attrs['dataset'] = 'era5'\n",
    "zg_attrs['detrended'] = '500 hPa vs surface temperature EurAR5 fldmean anom'\n",
    "zg_attrs['norm'] = 'latitudinal coefficient (eq1 from Dole and Gordon 1983)'\n",
    "zg_attrs['date_range'] = 'JJA 1979-2018'\n",
    "\n",
    "zg_dtrnd_norm_concat = xarray.concat(zg_dtrnd_norm, \"latitude\").transpose('time', 'latitude', 'longitude')\n",
    "#save here to then use cdo timstd to calculate the time standard deviation to create the array of normalized anomalies\n",
    "#(see 2.2.4 in Pinheiro et al. (2019))\n",
    "zg_dtrnd_norm_concat.to_netcdf(\"/rds/general/project/carl_phd/live/carl/data/era5/day/zg/LTDM/\"\n",
    "                           \"500zg_JJA_era5_1979-2019_daymean_Eurplus5_1x1_LTDManom_dtrnd_wrt_tas_DG83.nc\")\n",
    "\n",
    "\n",
    "\n",
    "#(\"/rds/general/project/carl_phd/live/carl/data/era5/day/zg/tot/\"\n",
    "#                               \"500zg_JJA_era5_1979-2019_daymean_EurAR5_1x1_LTDManom_dtrnd_wrt_tas_DG83.nc\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then run ``cdo timstd /rds/general/project/carl_phd/live/carl/data/era5/day/zg/500zg_JJA_era5_1979-2019_daymean_EurAR5_1x1_anom_dtrnd_wrt_tas_DG83.nc /rds/general/project/carl_phd/live/carl/data/era5/day/zg/500zg_JJA_era5_1979-2019_daymean_EurAR5_1x1_anom_dtrnd_wrt_tas_DG83_timstd.nc`` on the dataset to calculate the time std efficiently (it may be worth using the subprocess library to integrate the bash script commands with this ipynb) to implement the normalized anomaly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.DataArray '500zg' (time: 3772, latitude: 46, longitude: 51)>\n",
       "array([[[ -19.024953,  -14.202639, ...,   15.995887,   12.627666],\n",
       "        [  -9.046545,   -3.793881, ...,   20.378644,   16.193107],\n",
       "        ...,\n",
       "        [-121.58304 , -117.623154, ..., -113.253671, -109.402385],\n",
       "        [-124.210894, -121.396968, ..., -116.11093 , -113.487592]],\n",
       "\n",
       "       [[ -74.006541,  -72.279716, ...,   67.6553  ,   67.604186],\n",
       "        [ -69.200245,  -67.249532, ...,   74.467147,   74.286479],\n",
       "        ...,\n",
       "        [-104.427953,  -90.650472, ..., -146.018487, -142.672662],\n",
       "        [-102.187412,  -88.509315, ..., -147.897308, -145.389288]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ -52.060881,  -51.875366, ...,  141.285607,  143.290525],\n",
       "        [ -73.9818  ,  -73.87225 , ...,  142.477941,  144.630154],\n",
       "        ...,\n",
       "        [ -45.666772,  -42.143968, ...,  -29.268138,  -27.856335],\n",
       "        [ -44.200086,  -42.988199, ...,  -38.585354,  -36.758202]],\n",
       "\n",
       "       [[ -25.381098,  -25.432213, ...,  101.905311,  106.16822 ],\n",
       "        [ -43.205409,  -42.835812, ...,  100.106393,  104.932708],\n",
       "        ...,\n",
       "        [ -76.535451,  -78.054513, ...,  -53.61477 ,  -53.70661 ],\n",
       "        [ -73.478798,  -74.607821, ...,  -60.705301,  -60.071391]]])\n",
       "Coordinates:\n",
       "  * time       (time) datetime64[ns] 1979-06-01T10:30:00 ... 2019-08-31T10:30:00\n",
       "  * longitude  (longitude) float32 -10.0 -9.0 -8.0 -7.0 ... 37.0 38.0 39.0 40.0\n",
       "  * latitude   (latitude) float32 75.0 74.0 73.0 72.0 ... 33.0 32.0 31.0 30.0"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zg_dtrnd_norm_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "timstd_arr = np.zeros((46,51))\n",
    "\n",
    "for i in range(46):\n",
    "    for j in range(51):\n",
    "        timstd_arr[i,j] = np.std(zg_dtrnd_norm_concat[:,i,j])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[66.4400665 , 66.70864304, 66.99215774, ..., 81.25366635,\n",
       "        81.54224194, 81.82385509],\n",
       "       [66.53622379, 66.80499571, 67.08299731, ..., 80.12357165,\n",
       "        80.42045015, 80.71467449],\n",
       "       [66.9259836 , 67.20141844, 67.48638179, ..., 79.02749678,\n",
       "        79.33280514, 79.64011473],\n",
       "       ...,\n",
       "       [55.37656294, 55.06929105, 54.51172693, ..., 44.25033883,\n",
       "        43.69257469, 43.07710164],\n",
       "       [53.68557145, 53.56784308, 53.1719918 , ..., 42.71974476,\n",
       "        42.1468252 , 41.50294956],\n",
       "       [51.78974339, 51.45974967, 50.75288479, ..., 41.40845938,\n",
       "        40.71139149, 40.11832077]])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timstd_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "#can use cdo timstd or manually calculate the tims std since there seem to be regridding issues\n",
    "zg_dtrnd_norm_concat_timstd = [np.std(zg_dtrnd_norm_concat[:,i,j]) for i in range(46) for j in range(51)]\n",
    "zg_dtrnd_norm_concat_timstd_ds = (xr.concat(zg_dtrnd_norm_concat_timstd, \"value\")).to_dataset()\n",
    "#convert to numpy array to apply the reshape operation\n",
    "norm_anom = zg_dtrnd_norm_concat_timstd_ds['500zg'].values.reshape(46,51)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.DataArray '500zg' (time: 1, latitude: 46, longitude: 51)>\n",
       "array([[[638.784487, 641.273553, ..., 770.289618, 772.654946],\n",
       "        [639.748873, 642.272347, ..., 758.300993, 760.731455],\n",
       "        ...,\n",
       "        [489.400102, 486.628736, ..., 399.542193, 393.707649],\n",
       "        [470.489308, 465.991472, ..., 388.13654 , 382.714808]]])\n",
       "Coordinates:\n",
       "  * longitude  (longitude) float32 -10.0 -9.0 -8.0 -7.0 ... 37.0 38.0 39.0 40.0\n",
       "  * latitude   (latitude) float32 75.0 74.0 73.0 72.0 ... 33.0 32.0 31.0 30.0\n",
       "  * time       (time) datetime64[ns] 1999-07-16T22:30:00"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xarray.open_dataset(\"/rds/general/project/carl_phd/live/carl/data/era5/day/zg/\"\n",
    "                    \"500zg_JJA_era5_1979-2019_daymean_EurAR5_1x1_anom_dtrnd_wrt_tas_DG83_timstd.nc\")['500zg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.Dataset>\n",
       "Dimensions:    (bnds: 2, latitude: 46, longitude: 360, time: 1)\n",
       "Coordinates:\n",
       "  * longitude  (longitude) float32 0.0 1.0 2.0 3.0 ... 356.0 357.0 358.0 359.0\n",
       "  * latitude   (latitude) float32 90.0 89.0 88.0 87.0 ... 48.0 47.0 46.0 45.0\n",
       "  * time       (time) datetime64[ns] 1999-07-16T22:30:00\n",
       "Dimensions without coordinates: bnds\n",
       "Data variables:\n",
       "    time_bnds  (time, bnds) datetime64[ns] ...\n",
       "    500zg      (time, latitude, longitude) float64 ...\n",
       "Attributes:\n",
       "    CDI:          Climate Data Interface version 1.9.0 (http://mpimet.mpg.de/...\n",
       "    Conventions:  CF-1.6\n",
       "    history:      Tue Jan 28 10:18:28 2020: cdo timstd 500zg_JJA_era5_1979-20...\n",
       "    CDO:          Climate Data Operators version 1.9.0 (http://mpimet.mpg.de/..."
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xarray.open_dataset(\"/rds/general/project/carl_phd/live/carl/data/era5/day/zg/tot/500zg_JJA_era5_1979-2019_daymean_EurAR5_1x1_LTDManom_dtrnd_wrt_tas_DG83_timstd.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_anom_orig = xarray.open_dataset(\"/rds/general/project/carl_phd/live/carl/data/era5/day/zg/\"\n",
    "                                \"500zg_JJA_era5_1979-2019_daymean_EurAR5_1x1_anom_dtrnd_wrt_tas_DG83_timstd.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[651.52192254, 654.15562704, 656.93581756, ..., 796.78645283,\n",
       "        799.61627122, 802.37781479],\n",
       "       [652.46485626, 655.10047669, 657.82660491, ..., 785.70456339,\n",
       "        788.61580142, 791.50101238],\n",
       "       [656.28690331, 658.98786146, 661.78225766, ..., 774.95627785,\n",
       "        777.95018043, 780.96370746],\n",
       "       ...,\n",
       "       [543.03143639, 540.01827911, 534.55071611, ..., 433.92590259,\n",
       "        428.45637824, 422.42095104],\n",
       "       [526.44930333, 525.29484012, 521.41305911, ..., 418.9166523 ,\n",
       "        413.29851141, 406.98456384],\n",
       "       [507.85851001, 504.62253885, 497.69090873, ..., 406.05797803,\n",
       "        399.22241873, 393.40667243]])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_anom*9.80616"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.DataArray (latitude: 46, longitude: 51)>\n",
       "array([[66.440067, 66.708643, 66.992158, ..., 81.253666, 81.542242, 81.823855],\n",
       "       [66.536224, 66.804996, 67.082997, ..., 80.123572, 80.42045 , 80.714674],\n",
       "       [66.925984, 67.201418, 67.486382, ..., 79.027497, 79.332805, 79.640115],\n",
       "       ...,\n",
       "       [55.376563, 55.069291, 54.511727, ..., 44.250339, 43.692575, 43.077102],\n",
       "       [53.685571, 53.567843, 53.171992, ..., 42.719745, 42.146825, 41.50295 ],\n",
       "       [51.789743, 51.45975 , 50.752885, ..., 41.408459, 40.711391, 40.118321]])\n",
       "Coordinates:\n",
       "  * latitude   (latitude) float32 75.0 74.0 73.0 72.0 ... 33.0 32.0 31.0 30.0\n",
       "  * longitude  (longitude) float32 -10.0 -9.0 -8.0 -7.0 ... 37.0 38.0 39.0 40.0"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_anom_xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_anom_xr = xr.DataArray(norm_anom, coords=(zg_dtrnd_norm_concat['latitude'], zg_dtrnd_norm_concat['longitude']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_anom_xr.to_netcdf(\"/rds/general/project/carl_phd/live/carl/data/era5/day/zg/tot/\"\n",
    "    \"500zg_JJA_era5_1979-2019_daymean_EurAR5_1x1_LTDManom_dtrnd_wrt_tas_DG83_timstd.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the file for the normalised anomalies\n",
    "#norm_anom = xarray.open_dataset(\"/rds/general/project/carl_phd/live/carl/data/era5/day/zg/\"\n",
    "#                                \"500zg_JJA_era5_1979-2019_daymean_EurAR5_1x1_anom_dtrnd_wrt_tas_DG83_timstd.nc\")\n",
    "norm_anom = xarray.open_dataset(\"/rds/general/project/carl_phd/live/carl/data/era5/day/zg/tot/\"\n",
    "                                \"500zg_JJA_era5_1979-2019_daymean_EurAR5_1x1_LTDManom_dtrnd_wrt_tas_DG83_timstd.nc\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply the varying anomaly threshold from Pinheiro et al. 2019\n",
    "#set values below minimum equal to minimum, as defined in P19\n",
    "#note that the signf here are opposite - we are concerned with very low PV anomalies instead of\n",
    "#high geopotential height anomalies\n",
    "amp_min_anom_thresh = 100\n",
    "var_anom_thresh_arr = norm_anom_xr[:,:]*1.5*9.80616\n",
    "var_anom_minthresh_arr = np.asarray(np.where(var_anom_thresh_arr < amp_min_anom_thresh))\n",
    "if var_anom_minthresh_arr.shape[1] > 0:\n",
    "    for var_anom_minthresh_lat, var_anom_minthresh_lon in zip(var_anom_minthresh_arr[0], var_anom_minthresh_arr[1]):\n",
    "        var_anom_thresh_arr[var_anom_minthresh_lat, var_anom_minthresh_lon] = amp_min_anom_thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.DataArray '500zg' (time: 3772, latitude: 46, longitude: 51)>\n",
       "array([[[ -19.024953,  -14.202639, ...,   15.995887,   12.627666],\n",
       "        [  -9.046545,   -3.793881, ...,   20.378644,   16.193107],\n",
       "        ...,\n",
       "        [-121.58304 , -117.623154, ..., -113.253671, -109.402385],\n",
       "        [-124.210894, -121.396968, ..., -116.11093 , -113.487592]],\n",
       "\n",
       "       [[ -74.006541,  -72.279716, ...,   67.6553  ,   67.604186],\n",
       "        [ -69.200245,  -67.249532, ...,   74.467147,   74.286479],\n",
       "        ...,\n",
       "        [-104.427953,  -90.650472, ..., -146.018487, -142.672662],\n",
       "        [-102.187412,  -88.509315, ..., -147.897308, -145.389288]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ -52.060881,  -51.875366, ...,  141.285607,  143.290525],\n",
       "        [ -73.9818  ,  -73.87225 , ...,  142.477941,  144.630154],\n",
       "        ...,\n",
       "        [ -45.666772,  -42.143968, ...,  -29.268138,  -27.856335],\n",
       "        [ -44.200086,  -42.988199, ...,  -38.585354,  -36.758202]],\n",
       "\n",
       "       [[ -25.381098,  -25.432213, ...,  101.905311,  106.16822 ],\n",
       "        [ -43.205409,  -42.835812, ...,  100.106393,  104.932708],\n",
       "        ...,\n",
       "        [ -76.535451,  -78.054513, ...,  -53.61477 ,  -53.70661 ],\n",
       "        [ -73.478798,  -74.607821, ...,  -60.705301,  -60.071391]]])\n",
       "Coordinates:\n",
       "  * time       (time) datetime64[ns] 1979-06-01T10:30:00 ... 2019-08-31T10:30:00\n",
       "  * longitude  (longitude) float32 -10.0 -9.0 -8.0 -7.0 ... 37.0 38.0 39.0 40.0\n",
       "  * latitude   (latitude) float32 75.0 74.0 73.0 72.0 ... 33.0 32.0 31.0 30.0"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zg_dtrnd_norm_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.DataArray (latitude: 46, longitude: 51)>\n",
       "array([[ 977.282884,  981.233441,  985.403726, ..., 1195.179679, 1199.424407,\n",
       "        1203.566722],\n",
       "       [ 978.697284,  982.650715,  986.739907, ..., 1178.556845, 1182.923702,\n",
       "        1187.251519],\n",
       "       [ 984.430355,  988.481792,  992.673386, ..., 1162.434417, 1166.925271,\n",
       "        1171.445561],\n",
       "       ...,\n",
       "       [ 814.547155,  810.027419,  801.826074, ...,  650.888854,  642.684567,\n",
       "         633.631427],\n",
       "       [ 789.673955,  787.94226 ,  782.119589, ...,  628.374978,  619.947767,\n",
       "         610.476846],\n",
       "       [ 761.787765,  756.933808,  746.536363, ...,  609.086967,  598.833628,\n",
       "         590.110009]])\n",
       "Coordinates:\n",
       "  * latitude   (latitude) float32 75.0 74.0 73.0 72.0 ... 33.0 32.0 31.0 30.0\n",
       "  * longitude  (longitude) float32 -10.0 -9.0 -8.0 -7.0 ... 37.0 38.0 39.0 40.0"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var_anom_thresh_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the boolean array showing the positive regions - can take a few seconds to run\n",
    "zg_day_bool_thresh = [zg_day*9.80616 > var_anom_thresh_arr for zg_day in zg_dtrnd_norm_concat[:]] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zg_day_bool_thresh_concat.rename({'__xarray_dataarray_variable__': '500zg'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatenate the lists of boolean DataArray objects - also takes a few seconds\n",
    "zg_day_bool_thresh_concat = xarray.concat(zg_day_bool_thresh, \"time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.DataArray (time: 3772, latitude: 46, longitude: 51)>\n",
       "array([[[False, False, ..., False, False],\n",
       "        [False, False, ..., False, False],\n",
       "        ...,\n",
       "        [False, False, ..., False, False],\n",
       "        [False, False, ..., False, False]],\n",
       "\n",
       "       [[False, False, ..., False, False],\n",
       "        [False, False, ..., False, False],\n",
       "        ...,\n",
       "        [False, False, ..., False, False],\n",
       "        [False, False, ..., False, False]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[False, False, ...,  True,  True],\n",
       "        [False, False, ...,  True,  True],\n",
       "        ...,\n",
       "        [False, False, ..., False, False],\n",
       "        [False, False, ..., False, False]],\n",
       "\n",
       "       [[False, False, ..., False, False],\n",
       "        [False, False, ..., False, False],\n",
       "        ...,\n",
       "        [False, False, ..., False, False],\n",
       "        [False, False, ..., False, False]]])\n",
       "Coordinates:\n",
       "  * longitude  (longitude) float32 -10.0 -9.0 -8.0 -7.0 ... 37.0 38.0 39.0 40.0\n",
       "  * latitude   (latitude) float32 75.0 74.0 73.0 72.0 ... 33.0 32.0 31.0 30.0\n",
       "  * time       (time) datetime64[ns] 1979-06-01T10:30:00 ... 2019-08-31T10:30:00"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zg_day_bool_thresh_concat#['time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'numpy.ndarray'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-164-d9dc1a0d5f8a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mxr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzg_day_bool_thresh_concat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/cmt3718/anaconda3/envs/odin/lib/python3.6/site-packages/xarray/core/dataset.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data_vars, coords, attrs, compat)\u001b[0m\n\u001b[1;32m    381\u001b[0m             \u001b[0mcoords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata_vars\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mcoords\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 383\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_init_vars_and_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_vars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m         \u001b[0;31m# TODO(shoyer): expose indexes as a public argument in __init__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/cmt3718/anaconda3/envs/odin/lib/python3.6/site-packages/xarray/core/dataset.py\u001b[0m in \u001b[0;36m_set_init_vars_and_dims\u001b[0;34m(self, data_vars, coords, compat)\u001b[0m\n\u001b[1;32m    394\u001b[0m         \"\"\"Set the initial value of Dataset variables and dimensions\n\u001b[1;32m    395\u001b[0m         \"\"\"\n\u001b[0;32m--> 396\u001b[0;31m         \u001b[0mboth_data_and_coords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_vars\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcoords\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mboth_data_and_coords\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m             raise ValueError('variables %r are found in both data_vars and '\n",
      "\u001b[0;32m/home/cmt3718/anaconda3/envs/odin/lib/python3.6/site-packages/xarray/core/dataset.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    394\u001b[0m         \"\"\"Set the initial value of Dataset variables and dimensions\n\u001b[1;32m    395\u001b[0m         \"\"\"\n\u001b[0;32m--> 396\u001b[0;31m         \u001b[0mboth_data_and_coords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_vars\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcoords\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mboth_data_and_coords\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m             raise ValueError('variables %r are found in both data_vars and '\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'numpy.ndarray'"
     ]
    }
   ],
   "source": [
    "xr.Dataset(zg_day_bool_thresh_concat.values)\n",
    "\n",
    "xr.Dataset(data_vars=None, coords=None, attrs=None, compat=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the attrs for the new xarray\n",
    "zg_attrs = OrderedDict()\n",
    "zg_attrs['units'] = 'm'\n",
    "zg_attrs['long_name'] = 'Geopotential height'\n",
    "zg_attrs['level'] = '500 hPa'\n",
    "zg_attrs['dataset'] = 'era5'\n",
    "zg_attrs['anom'] = 'subtract long term daily mean (see Pinheiro et al 2019)'\n",
    "zg_attrs['detrended'] = '500 hPa vs surface temperature EurAR5 fldmean anom'\n",
    "zg_attrs['norm'] = 'latitudinal coefficient (eq1 from Dole and Gordon 1983)'\n",
    "zg_attrs['date_range'] = 'JJA 1979-2018'\n",
    "zg_attrs['bool_thresh'] = 'True if value > 1.5*timstd or 100m (whichever is larger)'\n",
    "\n",
    "\n",
    "zg_day_bool_thresh_concat = xr.Dataset()\n",
    "zg_day_bool_thresh_concat.to_netcdf(\"/rds/general/project/carl_phd/live/carl/data/era5/day/zg/tot/\"\n",
    "                                    \"500zg_JJA_era5_1979-2019_daymean_EurAR5_1x1_LTDManom_dtrnd_wrt_tas_DG83_bool.nc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Apply the area threshold\n",
    "For both DG83 (500hPa geopotential height anomaly) and S04 (VPV anomaly).\n",
    "\n",
    "Define the relevant ``var_bool`` file depending on what one is calculating, and then the functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.DataArray '__xarray_dataarray_variable__' (time: 3772, latitude: 46, longitude: 51)>\n",
       "[8849112 values with dtype=bool]\n",
       "Coordinates:\n",
       "  * longitude  (longitude) float32 -10.0 -9.0 -8.0 -7.0 ... 37.0 38.0 39.0 40.0\n",
       "  * latitude   (latitude) float32 75.0 74.0 73.0 72.0 ... 33.0 32.0 31.0 30.0\n",
       "  * time       (time) datetime64[ns] 1979-06-01T10:30:00 ... 2019-08-31T10:30:00"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xr.open_dataset(\"/rds/general/project/carl_phd/live/carl/data/era5/day/zg/tot/\"\n",
    "                                    \"500zg_JJA_era5_1979-2019_daymean_EurAR5_1x1_LTDManom_dtrnd_wrt_tas_DG83_bool.nc\")['__xarray_dataarray_variable__']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.DataArray '500zg' ()>\n",
       "array(556593)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var_bool_orig.sum('time').sum('latitude').sum('longitude')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.DataArray '__xarray_dataarray_variable__' ()>\n",
       "array(572092)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var_bool.sum('time').sum('latitude').sum('longitude')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#these files have already had the amplitude calculation applied\n",
    "#var_bool = xarray.open_dataset(\"/rds/general/project/carl_phd/live/carl/data/era5/day/zg/\n",
    "#500zg_JJA_era5_1979-2018_0000_EurAR5_anom_dtrnd_wrt_tas_DG83_area_weighted_varbool.nc\")['500zg']\n",
    "\n",
    "var_bool_orig = xarray.open_dataset(\"/rds/general/project/carl_phd/live/carl/data/era5/day/zg/\"\n",
    "                \"500zg_JJA_era5_1979-2019_daymean_EurAR5_1x1_anom_dtrnd_wrt_tas_DG83_bool.nc\")['500zg']\n",
    "#for the redefined anomaly\n",
    "var_bool = xarray.open_dataset(\"/rds/general/project/carl_phd/live/carl/data/era5/day/zg/tot/\"\n",
    "                                    \"500zg_JJA_era5_1979-2019_daymean_EurAR5_1x1_LTDManom_dtrnd_wrt_tas_DG83_bool.nc\")['__xarray_dataarray_variable__']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "time, lat, lon = var_bool['time'], var_bool['latitude'], var_bool['longitude']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def len_iter(items):\n",
    "    return sum(1 for _ in items)\n",
    "\n",
    "def consecutive_one(data):\n",
    "    return max(len_iter(run) for val, run in groupby(data) if val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert lat and lon coordinates of polygon to calculate polygon area\n",
    "#from https://gis.stackexchange.com/questions/298929/converting-lat-lon-to-x-y-coordinates-with-pyproj\n",
    "#alternative solution: https://stackoverflow.com/questions/4681737/how-to-calculate-the-area-of-a-polygon-on-the-earths-surface-using-python/4682656#4682656\n",
    "P = pyproj.Proj(proj='utm', zone=31, ellps='WGS84', preserve_units=True)\n",
    "G = pyproj.Geod(ellps='WGS84')\n",
    "\n",
    "def LatLon_To_XY(Lat,Lon):\n",
    "    return np.array(P(Lat,Lon))    \n",
    "\n",
    "def XY_To_LatLon(x,y):\n",
    "    return P(x,y,inverse=True)    \n",
    "\n",
    "def distance(Lat1, Lon1, Lat2, Lon2):\n",
    "    return G.inv(Lon1, Lat1, Lon2, Lat2)[2]\n",
    "\n",
    "def area_of_polygon(x, y):\n",
    "    \"\"\"Calculates the area of an arbitrary polygon given its verticies\"\"\"\n",
    "    area = 0.0\n",
    "    for i in range(-1, len(x)-1):\n",
    "        area += x[i] * (y[i+1] - y[i-1])\n",
    "    return abs(area) / 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adapted from https://gis.stackexchange.com/questions/99917/converting-matplotlib-contour-objects-to-shapely-objects\n",
    "def find_nearest(array, value):\n",
    "    array = np.asarray(array)\n",
    "    idx = (np.abs(array - value)).argmin()\n",
    "    return idx+1\n",
    "\n",
    "def area_test(var_bool, lat, lon, grid_coords, area_thresh, prop_blocked_thresh, min_num_grid_cells):\n",
    "    \"\"\"\n",
    "    Calculate the polygon shapes, determine the area for the blocked polygons and if there is a\n",
    "    blocked polygon identify whether or not it passes the area_thresh threshold\n",
    "    if so, then day is blocked\n",
    "    \"\"\"\n",
    "    blocked_day = 0\n",
    "    prop_blocked = np.array(np.where(var_bool == 1)).shape[1]/(len(lat)*len(lon))\n",
    "    if prop_blocked < prop_blocked_thresh:\n",
    "        return blocked_day, [[0]]\n",
    "    #test if there are two few blocked grid cells that we know there cannot be blocking\n",
    "    CS = plt.contour(lon,lat,var_bool)\n",
    "    plt.close()\n",
    "    var_bool_blocked_idx = np.zeros((len(lat)*len(lon)))#use to identify blocked regions    \n",
    "    for col in CS.collections:\n",
    "        # Loop through all polygons that have the same intensity level\n",
    "        for contour_path in col.get_paths():\n",
    "            # Create the polygon for this intensity level\n",
    "            # The first polygon in the path is the main one, the following ones are \"holes\"\n",
    "            for ncp,cp in enumerate(contour_path.to_polygons()):\n",
    "                \n",
    "                try:\n",
    "                    x = cp[:,0]\n",
    "                    y = cp[:,1]\n",
    "                except TypeError: #when the data is stored as a list of arrays\n",
    "                    x = [arr[0] for arr in cp]\n",
    "                    y = [arr[1] for arr in cp]\n",
    "                new_shape = geometry.Polygon([(i[0], i[1]) for i in zip(x,y)])\n",
    "                if ncp == 0:\n",
    "                    poly = new_shape\n",
    "                else:\n",
    "                    # Remove the holes if there are any\n",
    "                    print(\"Holes\")\n",
    "                    poly = poly.difference(new_shape)\n",
    "                    # Can also be left out if you want to include all rings\n",
    "                # do something with polygon\n",
    "                # Extract the point values that define the perimeter of the polygon\n",
    "                lon_shape, lat_shape = poly.exterior.coords.xy\n",
    "                #make a path object for the polygon\n",
    "                tup_verts = list(zip(lat_shape, lon_shape))\n",
    "                p = Path(tup_verts) # make a polygon\n",
    "                points = grid_coords\n",
    "                grid = p.contains_points(points)\n",
    "                mask = grid.reshape(len(lat),len(lon)) # now you have a mask with points inside a polygon            \n",
    "                num_grid_cells_blocked_in_poly = sum(grid)\n",
    "                if num_grid_cells_blocked_in_poly > min_num_grid_cells:\n",
    "                    #blocked region potentially large enough to qualify as blocking\n",
    "                    #calculate area of polygon by converting from lat,lon coordinates to x, y coordinates\n",
    "                    x, y = LatLon_To_XY(lat_shape,lon_shape)\n",
    "                    area = geometry.Polygon(list(zip(x, y))).area\n",
    "                    #covnert from m^2 to km^2\n",
    "                    area_km = area/1e6\n",
    "                    if area_km > area_thresh:\n",
    "                        #label day as blocked\n",
    "                        blocked_day = 1\n",
    "                        #relabel cells in blocked polygon for testing the overlap critereon\n",
    "                        var_bool_blocked_idx += grid #add the array to a 1d level\n",
    "    #return if the day is blocked and the flattened index values of the blocked tiles for the overlap test\n",
    "    return blocked_day, np.where(var_bool_blocked_idx > 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#these variables can be modified\n",
    "area_thresh = 1e6 # minimum blocked area in km^2\n",
    "# 1e6 km^2 is a reasonable minimum area since this is approximately the squared radius of a typical anticyclone (pg 10 of Hoskins & James)\n",
    "grid_res = 1 #grid resolution\n",
    "#create a tuple of the lat/lon coordinates of the grid to test whether or not the grid cells exist within the point\n",
    "lats, lons = np.asarray(np.meshgrid(lat,lon))[0,:,:].flatten(), np.asarray(np.meshgrid(lat,lon))[1,:,:].flatten()\n",
    "grid_coords = list(zip(lats,lons))\n",
    "#rough minimum for the number of grid cells needed for a blocking event\n",
    "min_num_grid_cells=area_thresh*np.cos(np.radians(30))/(grid_res*grid_res*110*110) #0.25x0.25 grid x 110km (roughly 1/360 of Earth's circumference)\n",
    "#minimum proportion of grid cells to block\n",
    "prop_blocked_thresh = min_num_grid_cells/(len(lat)*len(lon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.DataArray '__xarray_dataarray_variable__' (time: 3772, latitude: 46, longitude: 51)>\n",
       "[8849112 values with dtype=bool]\n",
       "Coordinates:\n",
       "  * longitude  (longitude) float32 -10.0 -9.0 -8.0 -7.0 ... 37.0 38.0 39.0 40.0\n",
       "  * latitude   (latitude) float32 75.0 74.0 73.0 72.0 ... 33.0 32.0 31.0 30.0\n",
       "  * time       (time) datetime64[ns] 1979-06-01T10:30:00 ... 2019-08-31T10:30:00"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var_bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "750\n",
      "800\n",
      "850\n",
      "900\n",
      "950\n",
      "1000\n",
      "1050\n",
      "1100\n",
      "1150\n",
      "1200\n",
      "1250\n",
      "1300\n",
      "1350\n",
      "1400\n",
      "1450\n",
      "1500\n",
      "1550\n",
      "1600\n",
      "1650\n",
      "1700\n",
      "1750\n",
      "1800\n",
      "1850\n",
      "1900\n",
      "1950\n",
      "2000\n",
      "2050\n",
      "2100\n",
      "2150\n",
      "2200\n",
      "2250\n",
      "2300\n",
      "2350\n",
      "2400\n",
      "2450\n",
      "2500\n",
      "2550\n",
      "2600\n",
      "2650\n",
      "2700\n",
      "2750\n",
      "2800\n",
      "2850\n",
      "2900\n",
      "2950\n",
      "3000\n",
      "3050\n",
      "3100\n",
      "3150\n",
      "3200\n",
      "3250\n",
      "3300\n",
      "3350\n",
      "3400\n",
      "3450\n",
      "3500\n",
      "3550\n",
      "3600\n",
      "3650\n",
      "3700\n",
      "3750\n"
     ]
    }
   ],
   "source": [
    "#note that for this dataset this cell takes a long time (several minutes) to run\n",
    "lat = var_bool['latitude']\n",
    "lon = var_bool['longitude']\n",
    "time = var_bool['time']\n",
    "blocked_day_arr = []\n",
    "#save the block_arr_area and blocked_idx_vals files\n",
    "#this gives information about the index values that are blocked on that day, which will be used when applying the overlap threshold\n",
    "with open(f\"/rds/general/project/carl_phd/live/carl/data/era5/day/zg/block_data/blocked_day/LTDM/blocked_day_arr_tot_LTDManom.txt\", \"a+\") as blocked_day_arr_tot:\n",
    "    for i in range(0,len(time)):\n",
    "        if i%50 == 0:\n",
    "            print(i)\n",
    "        blocked_day, var_bool_blocked = area_test(var_bool[i,:,:], lat, lon, grid_coords, area_thresh, prop_blocked_thresh, min_num_grid_cells)\n",
    "        #the block_data files saves the data for each day for the particular indices that are blocked\n",
    "        np.savetxt(f\"/rds/general/project/carl_phd/live/carl/data/era5/day/zg/block_data/LTDM/blocked_idx_vals_LTDManom_%04d.txt\"%i, var_bool_blocked[0])        \n",
    "        blocked_day_arr_tot.write(\"%s \"%blocked_day)\n",
    "        \n",
    "blocked_day_arr_tot.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1809.0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(np.loadtxt(\"/rds/general/project/carl_phd/live/carl/data/era5/day/zg/block_data/blocked_day/LTDM/blocked_day_arr_tot_LTDManom.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "blocked_day_arr_tot = np.loadtxt(\"/rds/general/project/carl_phd/live/carl/data/era5/day/zg/block_data/blocked_day/blocked_day_arr_tot_newresdaymean.txt\")\n",
    "blocked_day_arr_tot = np.loadtxt(\"/rds/general/project/carl_phd/live/carl/data/era5/day/zg/block_data/blocked_day/LTDM/blocked_day_arr_tot_LTDManom.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Apply the persistence criterea\n",
    "This is defined and then applied both before and after the overlap criterea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_sequence_numpy(arr,seq):\n",
    "    \"\"\" Find sequence in an array using NumPy only.\n",
    "\n",
    "    Parameters\n",
    "    ----------    \n",
    "    arr    : input 1D array\n",
    "    seq    : input 1D array\n",
    "\n",
    "    Output\n",
    "    ------    \n",
    "    Output : 1D Array of indices in the input array that satisfy the \n",
    "    matching of input sequence in the input array.\n",
    "    In case of no match, an empty list is returned.\n",
    "    \"\"\"\n",
    "\n",
    "    # Store sizes of input array and sequence\n",
    "    Na, Nseq = len(arr), len(seq)\n",
    "\n",
    "    # Range of sequence\n",
    "    r_seq = np.arange(Nseq)\n",
    "\n",
    "    # Create a 2D array of sliding indices across the entire length of input array.\n",
    "    # Match up with the input sequence & get the matching starting indices.\n",
    "    M = (arr[np.arange(Na-Nseq+1)[:,None] + r_seq] == seq).all(1)\n",
    "\n",
    "    # Get the range of those indices as final output\n",
    "    if M.any() >0:\n",
    "        return np.where(np.convolve(M,np.ones((Nseq),dtype=int))>0)[0]\n",
    "    else:\n",
    "        return []         # No match found\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to apply the persstence critereon, need to remove all events that don't persist over a period of five days\n",
    "#any sequence of 1,1,1,1,1 or more is fine, but 0,1,1,1,1,0 or less must be removed\n",
    "#sequences to be removed: 0,1,1,1,1,0; 0,1,1,1,0; 0,1,1,0; 0,1,0\n",
    "#note that Verdecchia et al (1996) add some criterea for blocking to be removed\n",
    "def persistence_criterea(block_arr, persis_thresh, JJA_days):\n",
    "    \"\"\"\n",
    "    Apply the persistence criterea to a list of days which are blocked or not\n",
    "    by removing the sequences for days which are not blocked\n",
    "    persis_thresh is the minimum number of blocked days for the threshold (at least 2)\n",
    "    block_arr is an array of True/False days of blocking\n",
    "    NB removes when blocking occurs briefly at the start or end of a season\n",
    "    Assume that blocking at the end of August doesn't persist into early September\n",
    "    and that there was no blocking late May. This would be a sensible approximation to apply\n",
    "    since we are only here interested in JJA blocking events.\n",
    "    \"\"\"\n",
    "    #pad the array to remove any erroneous blocking events that would coincide from the end of the season to the start of the next\n",
    "    block_arr_pad = block_arr.copy()\n",
    "    #to add elements to an array, add from reverse so that the index numbering doesn't mess up\n",
    "    idx_arr = np.arange(JJA_days, block_arr.shape[0], JJA_days)[::-1]\n",
    "\n",
    "    for i in idx_arr:\n",
    "        block_arr_pad = np.concatenate((block_arr_pad[:i], [0], block_arr_pad[i:]))\n",
    "        \n",
    "    block_arr_persis=block_arr.copy()\n",
    "    #blocking sequences to be removed\n",
    "    seq_arr = [[0,1,0], [0,1,1,0], [0,1,1,1,0], [0,1,1,1,1,0], [0,1,1,1,1,1,0]]\n",
    "    seq_to_remove_arr = seq_arr[:persis_thresh-1]\n",
    "    \n",
    "\n",
    "    block_arr_pad_persis=block_arr_pad.copy()\n",
    "    for seq in seq_to_remove_arr:\n",
    "        seq_idx=search_sequence_numpy(block_arr_pad_persis,seq)\n",
    "        block_arr_pad_persis[seq_idx] = 0\n",
    "    # then need to remove the padding from the array\n",
    "    #indices to be removed - note the shift of the index ordering\n",
    "    added_idx = np.arange(JJA_days, block_arr.shape[0], JJA_days+1)\n",
    "    block_arr_persis = np.delete(block_arr_pad_persis, added_idx)\n",
    "    return block_arr_persis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the data from DG83\n",
    "block_idx_vals_file_list = sorted(glob.glob(\"/rds/general/project/carl_phd/live/carl/data/era5/day/zg/block_data/*blocked_idx_vals*.txt\"))\n",
    "block_idx_vals_file_list = sorted(glob.glob(\"/rds/general/project/carl_phd/live/carl/data/era5/day/zg/block_data/*newresdaymean*\"))\n",
    "block_idx_vals_file_list = sorted(glob.glob(\"/rds/general/project/carl_phd/live/carl/data/era5/day/zg/block_data/LTDM/*blocked_idx_vals*.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply persistence criterea\n",
    "#this seems to remove half of all blocking events and can take a few minutes to run\n",
    "blocked_day_arr_persis = persistence_criterea(np.array(blocked_day_arr_tot), persis_thresh, JJA_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3297985153764581"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(blocked_day_arr_persis)/len(blocked_day_arr_persis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Apply the overlap criterea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#potentially helpful: https://stackoverflow.com/questions/55641425/check-if-two-contours-intersect\n",
    "def overlap_contour_criterea(blocked_idx_vals, block_arr, persis_thresh, JJA_days, overlap_val):\n",
    "    \"\"\"\n",
    "    To calculate the overlap criterea, need contour overlap over the five day period\n",
    "    For each five day period, need contour overlap\n",
    "    for the region to remain blocked over that five day period.\n",
    "    \"\"\"\n",
    "    #block_arr = block_arr_orig.copy() #change back when not testing\n",
    "    for i, blocked in enumerate(block_arr):\n",
    "        # measure if the previous day was blocked\n",
    "        if i == 0: \n",
    "            prev_blocked = 0\n",
    "        else:\n",
    "            prev_blocked = block_arr[i-1]\n",
    "        if blocked == 1 and prev_blocked == 0: #if a new blocking event is starting\n",
    "            #measure the length of the blocked event (in days) by finding the first zero element after the first zero element\n",
    "            blocked_arr_samp = block_arr[i:]\n",
    "            length_of_block = next((index for index,value in enumerate(blocked_arr_samp) if value != 1), 0)\n",
    "            #need to sample periods of persis_thresh within this blocked period\n",
    "            #and test whether or not the contours overlap, that is whether or not a certain percentage of the\n",
    "            #indices of the blocked longitudes are identical\n",
    "            for k in range(i,i+length_of_block-persis_thresh+1):#loop within the blocked period across the minumum persistence criterea\n",
    "                #since the shorter periods have been removed, blocked_idx_arr shows the indices of all of the blocked zones on each day\n",
    "                #within the minimum time period\n",
    "                #blocked_idx_vals is a 1d list for the blocked indices\n",
    "                #calculate the number of tiles blocked across the period\n",
    "                blocked_across_period = calc_blocked_overlap_across_period(blocked_idx_vals[k:k+persis_thresh], overlap_val)\n",
    "                #if the number of blocked cells is less than the number of overlapping cells\n",
    "                #then there isn't a contour overlap across the five day period\n",
    "                #so need to find where the discontinuity takes place within this time period\n",
    "                if blocked_across_period < overlap_val:\n",
    "                    while blocked_across_period < overlap_val:\n",
    "                        for l in range(1, persis_thresh-1):\n",
    "                        #subselect periods of blocked days to find at what point the deviation occurs\n",
    "                        #and then eliminate previous day as a block\n",
    "                            blocked_idx_vals_subsamp_list = array_subsampled(blocked_idx_vals[k:k+persis_thresh-1], l)\n",
    "                            try:\n",
    "                                for m, blocked_idx_vals_subsamp in enumerate(blocked_idx_vals_subsamp_list):\n",
    "                                    block_arr[l+k-1+m] = 0\n",
    "                                    #when there is only one value in blocked_idx_vals_subsamp\n",
    "                                    #eliminate previous day as a block for each time when there isn't a block calculated across the period     \n",
    "                                    blocked_across_period = calc_blocked_overlap_across_period(blocked_idx_vals_subsamp, overlap_val)\n",
    "                            except TypeError: #exception when there is only one list\n",
    "                                    block_arr[l+k-1] = 0\n",
    "                                    blocked_across_period = calc_blocked_overlap_across_period(blocked_idx_vals_subsamp_list, overlap_val)\n",
    "                        blocked_across_period = overlap_val # to get out of the while loop if needed (at this point no overlap has been identified)\n",
    "    return block_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_blocked_overlap_across_period(blocked_idx_vals, overlap_val):\n",
    "    \"\"\"\n",
    "    Calculate the number of tiles blocked across period\n",
    "    \"\"\"\n",
    "    blocked_idx_across_period = []\n",
    "    for num_days, blocked_idx_day in enumerate(blocked_idx_vals):\n",
    "        blocked_idx_across_period = np.append(blocked_idx_across_period, blocked_idx_day)\n",
    "    #count the number of elements that occur num_days+1 times\n",
    "    blocked_idx_unique, return_counts_arr = np.unique(blocked_idx_across_period, return_counts=True)\n",
    "    blocked_across_period = len([j for i,j in enumerate(blocked_idx_unique) if return_counts_arr[i] > num_days])\n",
    "    return blocked_across_period\n",
    "\n",
    "def array_subsampled(arr, i):\n",
    "    \"\"\"\n",
    "    Return consecutive samples of an array\n",
    "    \"\"\"\n",
    "    arr_list = []\n",
    "    \n",
    "    #there are i numbers of array\n",
    "    for j in range(i+1):\n",
    "        #0:len(arr)-i, 1:len(arr)-i+1, ... , i-1:len(arr)-1, i:len(arr)\n",
    "        arr_list = np.append(arr_list, arr[0+j:len(arr)-i+j])\n",
    "    return arr_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate overlap - usually takes a few seconds\n",
    "blocked_day_arr_persis_overlap = overlap_contour_criterea(blocked_idx_vals, blocked_day_arr_persis, persis_thresh, JJA_days, overlap_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run the persistence threshold again to remove any blocking events that now no longer pass the persistence criterea\n",
    "blocked_day_arr_persis_overlap_persis = persistence_criterea(np.array(blocked_day_arr_persis_overlap), persis_thresh, JJA_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12963944856839874"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(blocked_day_arr_persis_overlap_persis)/len(blocked_day_arr_persis_overlap_persis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"/rds/general/user/cmt3718/home/data/DG83_idx/DG83_idx_block_arr_era5_daymean_LTDM.txt\"  , blocked_day_arr_persis_overlap_persis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TM90=np.loadtxt(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(534.0, 489.0)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(DG83_orig), sum(DG83_LTDM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DG83_LTDM=np.loadtxt(\"/rds/general/user/cmt3718/home/data/DG83_idx/DG83_idx_block_arr_era5_daymean_LTDM.txt\")\n",
    "DG83_orig=np.loadtxt(\"/rds/general/user/cmt3718/home/data/DG83_idx/DG83_idx_block_arr_era5_daymean.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  71,   93,   94,   95,   96,   97,   98,   99,  100,  121,  122,\n",
       "        123,  124,  125,  135,  136,  137,  138,  139,  221,  222,  223,\n",
       "        224,  225,  331,  521,  522,  523,  524,  525,  619,  620,  621,\n",
       "        622,  623,  624,  625,  626,  627,  628,  629,  656,  657,  658,\n",
       "        659,  660,  661,  663,  664,  665,  666,  667,  763,  764,  765,\n",
       "        766,  767,  768, 1205, 1206, 1207, 1208, 1209, 1260, 1261, 1262,\n",
       "       1263, 1264, 1265, 1266, 1267, 1268, 1412, 1413, 1414, 1415, 1416,\n",
       "       1572, 1573, 1574, 1630, 1631, 1632, 1633, 1634, 1635, 1636, 1637,\n",
       "       1638, 1649, 1650, 1651, 1652, 1653, 1654, 1655, 1694, 1695, 1701,\n",
       "       1702, 1703, 1704, 1727, 1728, 1739, 1868, 1941, 1942, 1943, 1944,\n",
       "       1945, 2068, 2069, 2070, 2071, 2072, 2073, 2074, 2075, 2076, 2120,\n",
       "       2121, 2192, 2358, 2367, 2368, 2369, 2370, 2371, 2372, 2373, 2641,\n",
       "       2668, 2669, 2670, 2671, 2672, 2673, 2674, 2720, 2822, 2823, 2824,\n",
       "       2825, 2826, 2827, 2828, 2829, 2891, 2892, 2893, 2894, 2895, 2896,\n",
       "       2904, 2905, 2906, 2907, 2908, 2909, 2910, 2911, 2912, 2913, 2914,\n",
       "       2916, 2917, 2918, 2919, 2920, 2921, 3023, 3024, 3025, 3026, 3027,\n",
       "       3028, 3221, 3222, 3223, 3224, 3225, 3255, 3262, 3267, 3268, 3274,\n",
       "       3275, 3283, 3284, 3285, 3286, 3287, 3446, 3447, 3448, 3449, 3450,\n",
       "       3633, 3634, 3635, 3636, 3637, 3759])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.where(DG83_orig[:] < DG83_LTDM[:])[0]) #list of days classified in LTDM which aren't in the original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([   0,    1,    2, ..., 3769, 3770, 3771]),)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(DG83_orig[:] == DG83_LTDM[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4885049010410959, 1.6785941972795775e-225)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scipy.stats.pearsonr(DG83_orig,DG83_LTDM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'xr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-c6ec2b14ce1a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mxr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/rds/general/user/cmt3718/home/data/UKESM1-0-LL_piControl/AGP/AGP_clusters_UKESM1-0-LL_r1i1p1f2_EUR_piControl_JJAextd_1960-2060.nc\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'TM90_clusters'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'xr' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:carl]",
   "language": "python",
   "name": "conda-env-carl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

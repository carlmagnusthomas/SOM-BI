{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SOM_CV_data_BMU+\n",
    "\n",
    "The SOM CV dataset but here adapting the choice of BMU to mask the regions of -ve anomalies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import netCDF4\n",
    "import matplotlib as mpl\n",
    "mpl.use('Agg', warn=False)\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import sys\n",
    "import pandas as pd\n",
    "import os\n",
    "os.environ[\"PROJ_LIB\"] = \"/rds/general/user/kc1116/home/anaconda3/envs/zeus/share/proj\"\n",
    "import matplotlib as mpl\n",
    "import matplotlib\n",
    "import matplotlib.colors as colors\n",
    "from matplotlib.ticker import MultipleLocator, FormatStrFormatter\n",
    "from matplotlib import rcParams\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import somoclu\n",
    "import cartopy\n",
    "import xarray\n",
    "import xarray as xr\n",
    "import glob\n",
    "import math\n",
    "from mpl_toolkits.basemap import Basemap as bm\n",
    "from itertools import groupby\n",
    "from scipy import stats\n",
    "import cartopy.crs as ccrs\n",
    "from statsmodels.stats.multitest import (multipletests, fdrcorrection,\n",
    "                                         fdrcorrection_twostage,\n",
    "                                         NullDistribution,\n",
    "                                         local_fdr)\n",
    "from scipy.stats import spearmanr, kendalltau, pearsonr\n",
    "from functools import wraps\n",
    "import errno\n",
    "import os\n",
    "import signal\n",
    "import SOM_trends_funcs as SOM_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "nrow_vals, ncol_vals = [6], [5]\n",
    "\n",
    "\n",
    "#fig, row and col have been added to make the code work on Carl's machine \n",
    "def plot_field(m, X, lats, lons, vmin, vmax, step, cmap, nrows = 2, ncols = 2, \n",
    "               ax=False, title=False, grid=False, fig=False, row=False, col=False, fname = None, fliplat = True):\n",
    "    if not ax: \n",
    "        f, ax = plt.subplots(figsize=(8, (X.shape[0] / float(X.shape[1])) * 8))\n",
    "    m.ax = ax\n",
    "    #print(f\"lons.shape, lats.shape = {lons.shape, lats.shape}\")        \n",
    "    llons, llats = np.meshgrid(np.array(lons), np.array(lats))  \n",
    "    if fliplat:\n",
    "        X = np.flip(np.array(X), 0)\n",
    "    #print(f\"X = {X}\")\n",
    "    im = m.contourf(llons, llats, np.array(X), np.arange(int(vmin), int(vmax)+int(step), int(step)),                     latlon=True, cmap=cmap, extend='both', ax=ax)\n",
    "    m.drawcoastlines()\n",
    "    if grid: \n",
    "        m.drawmeridians(np.arange(0, 360, 10), labels=[0,0,0,1])\n",
    "        m.drawparallels(np.arange(-80, 80, 10), labels=[1,0,0,0])\n",
    "    ##modified for this computer - set axes for colourbar manually\n",
    "    if not fig:\n",
    "        cbaxes = f.add_axes([0.925, 0.13, 0.02, 0.75])\n",
    "        f.colorbar(im, cax = cbaxes)#fraction=0.04, pad=0.04)\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        x_pos_arr_rows = [[0.45], [0.45,0.8725], [0.35,0.6225,0.89], [0.3,0.5,0.7,0.9], [0.3,0.45,0.6,0.75,0.8], [0.3,0.45,0.6,0.75,0.8]]\n",
    "        y_pos_arr_cols = [[0.20], [0.1225,0.535], [0.12,0.38,0.66], [0.1,0.3,0.5,0.7], [0.1,0.3,0.5,0.7], [0.1,0.25,0.4,0.55,0.7]]\n",
    "        width_arr = [0.01,0.01,0.01,0.02/3, 0.01]\n",
    "        height_arr = [0.20,0.30,0.345,0.345*2/3, 0.17]\n",
    "        try:\n",
    "            x_pos_arr = x_pos_arr_rows[ncols-1]\n",
    "            y_pos_arr = y_pos_arr_cols[nrows-1]        \n",
    "            height = height_arr[nrows-1]\n",
    "            width = width_arr[ncols-1]\n",
    "            cbaxes = fig.add_axes([x_pos_arr[col], y_pos_arr[row], width, height])\n",
    "        except: #here coded for nrows > ncols\n",
    "            x_pos_arr = np.linspace(0.3, 0.9, 12)\n",
    "            y_pos_arr = np.linspace(0.3, 0.9, 12)     \n",
    "            height = 0.17\n",
    "            width = 0.1\n",
    "            #if nrows > ncols:\n",
    "            cbaxes = fig.add_axes([x_pos_arr[col], y_pos_arr[row], width, height])\n",
    "            #if nrows < ncols:\n",
    "            #    cbaxes = fig.add_axes([x_pos_arr[row], y_pos_arr, width, height])                \n",
    "        #width = 0.01\n",
    "        #height = 0.345\n",
    "        fig.colorbar(im, cax = cbaxes)#fraction=0.04, pad=0.04)\n",
    "    if title:\n",
    "        ax.set_title(title)  \n",
    "    if fname != None:   \n",
    "        print(f\"saving {fname}\")\n",
    "        f.savefig(fname, dpi = 300, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Identify_BMU_from_codebook(codebook_da_reshaped, data_yr_reshaped, rownum, colnum):\n",
    "    \"\"\"\n",
    "    When calculating the best matching unit from a given codebook (use the codebook from reanalysis)\n",
    "    need to calculate the best matching units array\n",
    "    output an array specifying the BMU with [rownum, colnum] for each day in the dataset\n",
    "    \n",
    "    codebook_da - codebook as a DataArray\n",
    "    data_yr_reshaped - data from the xarray, reshaped so that the lat/lon values are along one axis\n",
    "    \"\"\"\n",
    "    bmu_arr=[]\n",
    "    for i, data_day in enumerate(data_yr_reshaped):\n",
    "        #if i < 20:\n",
    "            min_euclidean_distance = 1e12\n",
    "            #print(data_day.shape)\n",
    "            for rowcolnum, codebook in enumerate(codebook_da_reshaped):\n",
    "                #calculate Euclidean distance between codebook and day in dataset\n",
    "                euclidean_distance = float((((data_day-codebook)**2).sum())**0.5)\n",
    "                #float(((data_day-codebook)**2).sum())\n",
    "                if euclidean_distance < min_euclidean_distance:\n",
    "                    min_euclidean_distance = euclidean_distance\n",
    "                    min_rowcolnum = rowcolnum\n",
    "                #else:\n",
    "                #    print(f\"euclidean_distance, min_euclidean_distance = {euclidean_distance, min_euclidean_distance}\")\n",
    "            bmu_arr.append([min_rowcolnum//rownum, min_rowcolnum%colnum])\n",
    "    \n",
    "    return bmu_arr\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_SOM_data(zg_file, yrst, yrend, season, region, init, neigh, std, ep, rad_0, rad_N, rad_cooling, scale_0, scale_N, scale_cooling,\n",
    "            savefig_title, savefig_trends, n_rows, n_columns, lats_arr, lons_arr, samp, g, zg_str, colormap_str, suptitle, mult_test_method, #yrst=1979, yrend=2013\n",
    "                data_yr_reshaped, grid_res, vmin, vmax, step, lat_str, lon_str, units, mdl_ens_str, train_input = False, som_yr = None, JJA_days=104, BMU_pos=True, BMU_pos_thresh=0, save_SOM_str=False):\n",
    "    \"\"\"\n",
    "    Save the data for all the climate models\n",
    "    \"\"\"\n",
    "    #Get the prepared data\n",
    "    #issue with prepData function so load information beforehand\n",
    "    #lats, lons, dates, nt_yr, nr_lat, nr_lon, m = prepData(zg_file, yrst, yrend, \"JJA\", lats_arr, lons_arr, g, zg_str, grid_res, lat_str, lon_str)\n",
    "    lats, lons, dates = zg_file[lat_str], zg_file[lon_str], data_yr_reshaped['time']\n",
    "    m = bm(projection='cyl',llcrnrlat=lats[0],urcrnrlat=lats[-1],llcrnrlon=lons[0],urcrnrlon=lons[-1],resolution='l')\n",
    "    print(f\"save_SOM data_yr_reshaped.shape = {data_yr_reshaped.shape}\")\n",
    "    #Train the SOM if not inputting an already trained SOM pattern\n",
    "    if not BMU_pos: #using the classic definition of the best matching units\n",
    "        if train_input == False:\n",
    "            print(\"training\")\n",
    "            som_yr = somoclu.Somoclu(n_columns, n_rows, maptype=\"planar\",compactsupport=True,initialization=f\"{init}\", neighborhood=f\"{neigh}\", std_coeff=std)\n",
    "            som_yr.train(data_yr_reshaped,epochs=ep,radius0=rad_0,radiusN=rad_N,radiuscooling=f\"{rad_cooling}\",\n",
    "                     scale0=scale_0,scaleN=scale_N,scalecooling=f\"{scale_cooling}\")\n",
    "            bmus=som_yr.bmus\n",
    "        else:\n",
    "            #already have som_yr but need to calculate bmus\n",
    "            codebook_da = xarray.DataArray(som_yr.codebook, name = \"codebook\", dims = (\"row\", \"col\", \"latlon\"))\n",
    "            codebook_da_reshaped = codebook_da.values.reshape(n_rows*n_columns, data_yr_reshaped.shape[1])\n",
    "            codebook_da_reshaped_xr = xr.DataArray(codebook_da_reshaped, name = f\"codebook_reshaped\")\n",
    "            codebook_da_reshaped_xr = codebook_da_reshaped_xr.rename(dim_0=\"rowcol\").rename(dim_1=\"latlon_flat\")\n",
    "            bmus = Identify_BMU_from_codebook(codebook_da_reshaped_xr, data_yr_reshaped, n_rows, n_columns)\n",
    "    if BMU_pos:\n",
    "        if train_input == False:\n",
    "            print(\"training, BMU_pos\")\n",
    "            som_yr = somoclu.Somoclu(n_columns, n_rows, maptype=\"planar\",compactsupport=True,initialization=f\"{init}\", neighborhood=f\"{neigh}\", std_coeff=std)\n",
    "            som_yr.train(data_yr_reshaped,epochs=ep,radius0=rad_0,radiusN=rad_N,radiuscooling=f\"{rad_cooling}\",\n",
    "                     scale0=scale_0,scaleN=scale_N,scalecooling=f\"{scale_cooling}\")\n",
    "        #already have som_yr but need to calculate bmus\n",
    "        codebook_da = xarray.DataArray(som_yr.codebook, name = \"codebook\", dims = (\"row\", \"col\", \"latlon\"))\n",
    "        codebook_da_reshaped = codebook_da.values.reshape(n_rows*n_columns, data_yr_reshaped.shape[1])\n",
    "        codebook_da_reshaped_xr = xr.DataArray(codebook_da_reshaped, name = f\"codebook_reshaped\")\n",
    "        codebook_da_reshaped_xr = codebook_da_reshaped_xr.rename(dim_0=\"rowcol\").rename(dim_1=\"latlon_flat\")\n",
    "        bmus = Identify_BMU_from_codebook_posanom(codebook_da_reshaped_xr, data_yr_reshaped, n_rows, n_columns, BMU_pos_thresh)\n",
    "\n",
    "    #Plot the trained SOM nodes \n",
    "    g, axes = plt.subplots(nrows=n_rows, ncols=n_columns, figsize=(10,5))\n",
    "    g.subplots_adjust(hspace=0.2, wspace=0.2)\n",
    "    #Store the frequency data in a list\n",
    "    freq_list = []\n",
    "    for i in range(n_rows): \n",
    "        for j in range(n_columns):\n",
    "            try:\n",
    "                ax = axes[i][j]\n",
    "                node = som_yr.codebook[i][j]\n",
    "            except: #doesn't work when not indexed, when one of n_rows or n_columns = 1\n",
    "                if n_rows > n_columns:\n",
    "                    ax = axes[i]\n",
    "                    node = som_yr.codebook[j]\n",
    "                else:\n",
    "                    ax = axes[i]\n",
    "                    node = som_yr.codebook[i]\n",
    "            #print(f\"node = {node}\")\n",
    "            #print(f\"node.shape = {node.shape}\")\n",
    "            node_orig = node.reshape(len(lats),len(lons))\n",
    "            \n",
    "            plot_field(m, node_orig, lats, lons, -330, 330, 30, ncols = n_columns, nrows = n_rows,  #for pv use -330/250, 330/250, 30/250 as levels\n",
    "                       ax=ax, grid=False, cmap=plt.get_cmap(colormap_str), fig=g, row=i, col=j)\n",
    "            #Pattern frequencies\n",
    "            if train_input == False:\n",
    "                freq = sum([1 for pat in som_yr.bmus if pat.tolist() == [j,i]])/float(len(som_yr.bmus))*100 #note the flipped indices\n",
    "            else:\n",
    "                #print(f\"pat = {[j,i]}\")\n",
    "                freq = sum([1 for pat in bmus if pat == [j,i]])/float(len(bmus))*100 #note the flipped indices\n",
    "                #print(f\"freq = {freq}\")\n",
    "            freq_list.append(freq)\n",
    "            props = dict(boxstyle='round', facecolor='white', alpha=0.8)\n",
    "            ax.text(0.05, 0.95, str(round(freq,1))+\"%\", transform=ax.transAxes, fontsize=14, verticalalignment='top', bbox=props)\n",
    "            #ax1.ticklabel_format(style=\"plain\")      \n",
    "    suptitle=f\"zg LTDM anom {mdl_ens_str} {yrst}-{yrend} JJA ({units})\"            \n",
    "    g.suptitle(f\"{suptitle}\", fontsize=10)    \n",
    "    ax.ticklabel_format(style=\"plain\")      \n",
    "    if save_SOM_str != False:\n",
    "        print(f\"save_SOM_str = {save_SOM_str}\")\n",
    "        g.savefig(save_SOM_str, bbox_inches=\"tight\", dpi = 300)\n",
    "    plt.close();\n",
    "        \n",
    "    da_xr_trend, all_occur = saveSOMTrends(som_yr, dates, n_rows, n_columns, savefig_trends, colormap_str,\n",
    "                                mult_test_method, train_input=train_input, bmus=bmus, JJA_days=JJA_days, yrst=yrst, yrend=yrend, samp=samp)\n",
    "    return da_xr_trend, all_occur, som_yr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify the number of years selected for cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Modify the number of years selected for cross validation\n",
    "\n",
    "#Function to plot the SOM trends\n",
    "#added values to modify Gerald's function (issues with date formatting)\n",
    "def saveSOMTrends(som_yr, dates, n_rows, n_columns, savefig_trends, colormap_str, mult_test_method, \n",
    "                  train_input=False, bmus=None, JJA_days=104, yrst=1979, yrend=2019, samp=50):\n",
    "    \"\"\"\n",
    "    Plots the trends (occurrence, persistence, max duration) of the SOM patterns generated from the GPH data. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    som_yr    : somoclu.train.Somoclu\n",
    "        The trained SOM (somoclu object)\n",
    "    seas_yr   : xarray.DataArray\n",
    "        Data array containing GPH data and information on time, latitude, longitude variables in the selected time range, season and region\n",
    "        \n",
    "    Returns\n",
    "    ---------\n",
    "    grad_list : list \n",
    "        List of the gradients of the pattern occurrence trends computed by linear least-squares regression \n",
    "    all_occur : list \n",
    "        List of occurrences per day in the trained dataset for each SOM pattern\n",
    "    \"\"\"\n",
    "    if train_input==False:\n",
    "        #calculate the best matching unit\n",
    "        #Extract pattern data (occurence, persistence, max duration)\n",
    "        #Store each pattern data as a binary list of len(dates), i.e. [1,0,0,0,1...,1]\n",
    "        bmus = som_yr.bmus\n",
    "    \n",
    "    \n",
    "    global all_occur #Extra code to store the occurences in a global variable that I can access later\n",
    "    #print(f\"bmus = {bmus}\")\n",
    "    #print(f\"bmus.shape = {bmus.shape}\")    \n",
    "    all_occur = [[] for i in range(max(n_rows,n_columns))]\n",
    "    for a in range(n_rows):\n",
    "        for b in range(n_columns): \n",
    "            curr_occur = []\n",
    "            for c in bmus:\n",
    "                if train_input==False:\n",
    "                    if c.tolist() == [b,a]: #### need to swap the indices here!! #b'a\n",
    "                        curr_occur.append(1) #This pattern occured on this date\n",
    "                    else:\n",
    "                        curr_occur.append(0) #This pattern didn't occur \n",
    "                else:\n",
    "                    if c == [b,a]: #### need to swap the indices here!! #b'a\n",
    "                        curr_occur.append(1) #This pattern occured on this date\n",
    "                    else:\n",
    "                        curr_occur.append(0) #This pattern didn't occur                     \n",
    "            all_occur[a].append(curr_occur)\n",
    "    #Loop through each set of circulation occurence data \n",
    "    #print(f\"dates.values = {dates.values}\")\n",
    "    #print(f\"all_occur.shape = {np.array(all_occur).shape}\")\n",
    "    #print(f\"dates.shape = {np.array(dates).shape}\")\n",
    "    \n",
    "    #print(f\"styr, endyr = {styr, endyr}\")\n",
    "    da_arr, rownum_arr, colnum_arr = [], [], []\n",
    "    \n",
    "    for row in range(n_rows):  \n",
    "        for col in range(n_columns):\n",
    "            #print(f\"row, col = {row, col}\")\n",
    "            #Need some exception handling for rare cases where any of the patterns doesnt occur in a year\n",
    "            #NOTE the swapped indices [col][row] here! Doulbe check with Joy's code, see if node number can be confirmed\n",
    "            #all_occur[row][col] is the relevant matrix for investigating case studies\n",
    "            #print(f\"all_occur = {all_occur}\")\n",
    "            cum_data = pd.Series(np.array(all_occur[row][col]), index=dates.values)\n",
    "            #print(f\"cum_data = {cum_data}\")\n",
    "            #to generate the years in the dataset, since cum_data.index.year has issues\n",
    "            years_num = yrend - yrst\n",
    "            ##JJA_days = 104       \n",
    "            #print(f\"cum_data.shape = {cum_data.shape}\")\n",
    "            years_num = int(cum_data.shape[0]/JJA_days)\n",
    "            #print(f\"years_num = {years_num}\")\n",
    "            years_zeros = np.zeros((years_num*JJA_days))\n",
    "            if type(yrst) == int or type(yrst) == float:\n",
    "                arr_gen = np.array([years_zeros[JJA_days*i:JJA_days*(i+1)]+i+yrst for i in range(years_num)\n",
    "                               ]).reshape(years_num*JJA_days)\n",
    "            else: #yrst an array\n",
    "                arr_gen = np.array([years_zeros[JJA_days*i:JJA_days*(i+1)]+i+int(yrst.values) for i in range(years_num)\n",
    "                               ]).reshape(years_num*JJA_days)            \n",
    "\n",
    "            sort_to_year = cum_data.groupby(arr_gen).apply(list)\n",
    "            year = sort_to_year.keys().tolist() #[1979, 1980,...]\n",
    "            occ = [sum(d) for d in sort_to_year]  #[25,40,...]\n",
    "            # Count streaks \n",
    "            streaks = [[sum(1 for i in g) for k,g in groupby(x) if k == 1] for x in sort_to_year]\n",
    "            streak_count = [[(k, sum(1 for i in g)) for k,g in groupby(sorted(x)) ] for x in streaks]\n",
    "            \n",
    "            #Persistence (if statement to avoid division by zero )\n",
    "            persis = [ sum([x[0]*x[1] for x in year])/sum([x[1] for x in year]) if sum([x[1] for x in year]) != 0 else 0 for year in streak_count ]\n",
    "            #Max Duration \n",
    "            max_dur = [ year[-1][0] if year != [] else 0 for year in streak_count]\n",
    "            ##want to store occ, persis, max_dur and ev_no in a xarray Dataset/DataArray to save, with the time file showing the years\n",
    "            ##need to store this for each node (36 different 1d arrays for each model, plus the timestamp)\n",
    "\n",
    "            def len_iter(items):\n",
    "                return sum(1 for _ in items)            \n",
    "            \n",
    "            def consecutive_one_len(data):\n",
    "                return len(list(len_iter(run) for val, run in groupby(data) if val))            \n",
    "            event_no = [consecutive_one_len(d) for d in sort_to_year] \n",
    "            \n",
    "            store = [occ, persis, max_dur, event_no]\n",
    "            \n",
    "            da = xarray.DataArray(store, dims = ('SOM_trend_metrics', \"year\"))\n",
    "            da['year'] = year\n",
    "            da['SOM_trend_metrics'] = ['occ', 'persis', 'max_dur', 'event_no']\n",
    "            da_arr.append(da)\n",
    "            #store the DataArray and the accompanying row and column number of the SOM pattern            \n",
    "            rownum_arr.append(row)\n",
    "            colnum_arr.append(col)\n",
    "            \n",
    "    #concatenate the array\n",
    "    da_xr = xarray.concat(da_arr, dim = (\"node\"))\n",
    "    #add a new coordinates to specify the row and column number for each layer\n",
    "    da_xr = da_xr.assign_coords(\n",
    "        rownum=('node', rownum_arr))\n",
    "    da_xr = da_xr.assign_coords(\n",
    "        colnum=('node', colnum_arr))            \n",
    "            \n",
    "    return da_xr, all_occur\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def SOM_calc_samp(file_zg_str, n_rows, n_columns, yrnum_train, BMU_pos=True, BMU_pos_thresh=0):\n",
    "    \"\"\"\n",
    "    Calculate and save the trained arrangement of SOM nodes for a set of subsamples of the data for the purposes of cross-validation \n",
    "    \"\"\"\n",
    "    if BMU_pos:\n",
    "        BMU_pos_str = \"_BMU+\"\n",
    "    else:\n",
    "        BMU_pos_str = \"\"\n",
    "    \n",
    "    \n",
    "    file_zg_tot = xarray.open_dataset(file_zg_str).squeeze()\n",
    "    \n",
    "    yrst, yrend = int(file_zg_tot['time.year'].min()), int(file_zg_tot['time.year'].max())\n",
    "    yrst_train_arr = [yrst+i*yrnum_train for i in range(int((yrend - yrst)/yrnum_train))]\n",
    "    \n",
    "    k = int((yrend - yrst)/yrnum_train)\n",
    "    #print(f\"{k}-fold cv\")    \n",
    "    if \"psl\" in file_zg_str:\n",
    "        zg_str = \"psl\"\n",
    "        units = \"Pa\"\n",
    "        data_yr_reshaped_str = (\"/rds/general/user/cmt3718/home/data/cmip6/UKESM1-0-LL/piControl/r1i1p1f2/psl/\"\n",
    " \"psl_day_UKESM1-0-LL_piControl_r1i1p1f2_gn_19600101-20601230_r180x91_LTDManom_EUR2_JJAextd_dtrnd_reshaped.nc\")     \n",
    "    if \"msl\" in file_zg_str:\n",
    "        zg_str = \"msl\"\n",
    "        units = \"Pa\"\n",
    "        data_yr_reshaped_str = f\"/rds/general/project/nowack_graven/live/carl/era5/mean_sea_level_pressure/mslp_ERA5_{yrst}-{yrend}_EUR_JJAextd_LTDMdaymean_anom_reshaped.nc\"\n",
    "    if \"zg\" in file_zg_str:\n",
    "        zg_str = \"z\"\n",
    "        units = \"hPa\"\n",
    "        data_yr_reshaped_str = \"/rds/general/project/carl_phd/live/carl/data/era5/day/zg/LTDM/z_timedtrnd_ERA5_1979-2019_EUR_JJAextd_LTDMdaymean_anom_sort_reshaped.nc\"\n",
    "        #(\"/rds/general/project/carl_phd/live/carl/data/era5/day/zg/LTDM/500zg_1x1_1979-2019_JJAextd_LTDManom_nolp_EurAR5_timedtrnd_reshaped.nc\")\n",
    "        #f\"/rds/general/project/carl_phd/live/carl/data/era5/day/zg/LTDM/z_timedtrnd_ERA5_{yrst}-{yrend}_EUR_JJAextd2_LTDMdaymean_anom_reshaped.nc\"\n",
    "    if \"UKESM\" in file_zg_str:\n",
    "        if \"zg\" in file_zg_str:\n",
    "            zg_str = \"zg\"\n",
    "            data_yr_reshaped_str = (\"/rds/general/user/cmt3718/home/data/cmip6/UKESM1-0-LL/piControl/r1i1p1f2/zg/\"\n",
    "                \"500zg_day_UKESM1-0-LL_piControl_r1i1p1f2_gn_19600101-20601230_r180x91_LTDManom_EUR_JJAextd_dtrnd_reshaped.nc\")\n",
    "            data_yr_reshaped = xr.open_dataset(data_yr_reshaped_str)['data_yr_reshaped']\n",
    "        if \"psl\" in file_zg_str:\n",
    "            zg_str = \"psl\"\n",
    "            data_yr_reshaped_str = (\"/rds/general/user/cmt3718/home/data/cmip6/UKESM1-0-LL/piControl/r1i1p1f2/psl/\"\n",
    "                \"psl_day_UKESM1-0-LL_piControl_r1i1p1f2_gn_19600101-20601230_r180x91_LTDManom_EUR_JJAextd_dtrnd_reshaped.nc\")\n",
    "            data_yr_reshaped = xr.open_dataset(data_yr_reshaped_str)['data_yr_reshaped']      \n",
    "    if \"era5\" in data_yr_reshaped_str:\n",
    "          data_yr_reshaped = xr.open_dataset(data_yr_reshaped_str)[f'era5_{zg_str}_reshaped']            \n",
    "    file_zg_tot = file_zg_tot[zg_str]\n",
    "\n",
    "    lat_str, lon_str = \"latitude\", \"longitude\"\n",
    "    if \"era5\" in file_zg_str:\n",
    "        mdl, ens = \"era5\", \"reanal\"\n",
    "        mdl_ens_str = f\"{mdl}_{ens}\"\n",
    "        mip = \"reanal\"\n",
    "    if \"UK\" in file_zg_str:\n",
    "        mdl, ens = \"UKESM1-0-LL\", \"r1i1p1f2\"\n",
    "        mdl_ens_str = f\"{mdl}_{ens}\"\n",
    "        mip = \"cmip6\"\n",
    "        lat_str, lon_str = \"lat\", \"lon\"\n",
    "    lats_arr = [30,75]\n",
    "    lons_arr = [-10,40]\n",
    "    grid_res = 1\n",
    "    lats, lons = np.arange(30,76), np.arange(-10,41)\n",
    "    #need to adjust the longitude coordinate if it runs from 0 - 360 E instead of -180 to +180 E\n",
    "    file_zg_tot=SOM_fn.da_lon_adj(file_zg_tot)\n",
    "    if \"UKESM\" in file_zg_str:\n",
    "        file_zg_tot = file_zg_tot.sel(time = np.isin(file_zg_tot['time.dayofyear'], np.arange(147,245)))\n",
    "    else:\n",
    "        JJA_extd_daymonth_xr = xr.open_dataset(\"/rds/general/project/nowack_graven/live/carl/era5/day_month_1979-2019_JJAextd.nc\")['JJA_extd']\n",
    "        #zg_file['day_month'] = xr.open_dataset(\"/rds/general/project/nowack_graven/live/carl/era5/day_month_1979-2019_nolp.nc\")['day_month']\n",
    "        day_month = xr.open_dataset(\"/rds/general/project/nowack_graven/live/carl/era5/day_month_1979-2019_nolp.nc\")['day_month']\n",
    "        file_zg_tot = file_zg_tot.sel(latitude = np.isin(file_zg_tot['latitude'], lats), \n",
    "                                  longitude = np.isin(file_zg_tot['longitude'], lons))\n",
    "        if \"msl\" in file_zg_str:\n",
    "            file_zg_tot = file_zg_tot.sel(time = np.isin(day_month, JJA_extd_daymonth_xr))\n",
    "    num_days, num_lats, num_lons = file_zg_tot.shape\n",
    "\n",
    "\n",
    "\n",
    "    vmin, vmax, step = -15, 15, 1.5\n",
    "    init, neigh, std, ep, rad_0, rad_N, rad_cooling, scale_0, scale_N, scale_cooling = \"pca\", \"gaussian\", 0.5, 50, 1, 0, \"linear\", 0.1, 0.01, \"exponential\"\n",
    "    region, season = \"Europe\", \"JJA_extd\"\n",
    "    domain=\"EUR\"\n",
    "    samp = 15\n",
    "    colormap_str = \"seismic\"\n",
    "    suptitle=f\"zg LTDM anom {mdl} {yrst}-{yrend} JJA extd ({units})\"\n",
    "    #all cover different variations on the mltiple hypothesis test applied in Horton et al 2015\n",
    "    mult_test_method = \"fdr_bh\"#\"k-FWER\"#\"none\"#\"fdr_bh\"\n",
    "    g=9.80665\n",
    "\n",
    "    #cross-validation training period\n",
    "    for yrst_train in yrst_train_arr[:]:\n",
    "        print(f\"train period {yrst_train}-{yrst_train+yrnum_train}\")\n",
    "        if int(yrst_train) > 0:\n",
    "            #subselect training and cv periods    \n",
    "            file_zg_tot_train = file_zg_tot.sel(time = np.isin(file_zg_tot['time.year'], np.arange(yrst_train, yrst_train+yrnum_train+1), invert=True))\n",
    "            data_yr_reshaped_train = data_yr_reshaped.sel(time = np.isin(data_yr_reshaped['time'], file_zg_tot_train['time']))\n",
    "\n",
    "            file_zg_tot_cv = file_zg_tot.sel(time = np.isin(file_zg_tot['time.year'], np.arange(yrst_train, yrst_train+yrnum_train+1), invert=False))\n",
    "            data_yr_reshaped_cv = data_yr_reshaped.sel(time = np.isin(data_yr_reshaped['time'], file_zg_tot_cv['time']))  \n",
    "            \n",
    "            \n",
    "            if \"UKESM\" in file_zg_str:\n",
    "                #/rds/general/user/cmt3718/home/data/cmip6/SOM/trends/UK-ESM1-0-LL_piControl/zg/crossval/10-fold\n",
    "                #/rds/general/project/nowack_graven/live/carl_som_index/data/UKESM1-0-LL_piControl/zg/crossval/5-fold/\n",
    "                savefig_SOM = f\"/rds/general/project/nowack_graven/live/carl_som_index/data/UKESM1-0-LL_piControl/{zg_str}/crossval/{k}-fold/plots/SOM_fig_{mdl_ens_str}_{n_rows}x{n_columns}_{domain}_not{int(yrst_train)}-{int(yrst_train+yrnum_train)}_{season}_{zg_str}_LTDManom{BMU_pos_str}.png\"\n",
    "                savefig_trends = f\"/rds/general/project/nowack_graven/live/carl_som_index/data/UKESM1-0-LL_piControl/{zg_str}/crossval/{k}-fold/plots/SOMtrends_fig_{mdl_ens_str}_{n_rows}x{n_columns}_{domain}_not{int(yrst_train)}-{int(yrst_train+yrnum_train)}_{season}_{zg_str}_LTDManom{BMU_pos_str}.png\"\n",
    "                SOM_trend_str_train = f\"/rds/general/project/nowack_graven/live/carl_som_index/data/UKESM1-0-LL_piControl/{zg_str}/crossval/{k}-fold/SOM_train_{mdl_ens_str}_{n_rows}x{n_columns}_{domain}_not{int(yrst_train)}-{int(yrst_train+yrnum_train)}_{season}_{zg_str}_LTDManom{BMU_pos_str}.nc\"\n",
    "                SOM_data_occ_str_train = f\"/rds/general/project/nowack_graven/live/carl_som_index/data/UKESM1-0-LL_piControl/{zg_str}/crossval/{k}-fold/SOM_data_occur_train_{mdl_ens_str}_{n_rows}x{n_columns}_{domain}_not{int(yrst_train)}-{int(yrst_train+yrnum_train)}_{season}_{zg_str}_LTDManom{BMU_pos_str}.nc\"\n",
    "                SOM_trend_str_cv = f\"/rds/general/project/nowack_graven/live/carl_som_index/data/UKESM1-0-LL_piControl/{zg_str}/crossval/{k}-fold/SOM_cv_{mdl_ens_str}_{n_rows}x{n_columns}_{domain}_{int(yrst_train)}-{int(yrst_train+yrnum_train)}_{season}_{zg_str}_LTDManom{BMU_pos_str}.nc\"\n",
    "                SOM_data_occ_str_cv = f\"/rds/general/project/nowack_graven/live/carl_som_index/data/UKESM1-0-LL_piControl/{zg_str}/crossval/{k}-fold/SOM_data_occur_cv_{mdl_ens_str}_{n_rows}x{n_columns}_{domain}_{int(yrst_train)}-{int(yrst_train+yrnum_train)}_{season}_{zg_str}_LTDManom{BMU_pos_str}.nc\"\n",
    "            \n",
    "            elif \"era5\" in file_zg_str:\n",
    "                savefig_SOM = f\"/rds/general/project/nowack_graven/live/carl_som_index/data/era5/{zg_str}/crossval/{k}-fold/plots/SOM_fig_{mdl_ens_str}_{n_rows}x{n_columns}_{domain}_not{int(yrst_train)}-{int(yrst_train+yrnum_train)}_{season}_{zg_str}_LTDManom{BMU_pos_str}.png\"\n",
    "                savefig_trends = f\"/rds/general/project/nowack_graven/live/carl_som_index/data/era5/{zg_str}/crossval/{k}-fold/plots/SOMtrends_fig_{mdl_ens_str}_{n_rows}x{n_columns}_{domain}_not{int(yrst_train)}-{int(yrst_train+yrnum_train)}_{season}_{zg_str}_LTDManom{BMU_pos_str}.png\"\n",
    "                SOM_trend_str_train = f\"/rds/general/project/nowack_graven/live/carl_som_index/data/era5/{zg_str}/crossval/{k}-fold/SOM_train_{mdl_ens_str}_{n_rows}x{n_columns}_{domain}_not{int(yrst_train)}-{int(yrst_train+yrnum_train)}_{season}_{zg_str}_LTDManom{BMU_pos_str}.nc\"\n",
    "                SOM_data_occ_str_train = f\"/rds/general/project/nowack_graven/live/carl_som_index/data/era5/{zg_str}/crossval/{k}-fold/SOM_data_occur_train_{mdl_ens_str}_{n_rows}x{n_columns}_{domain}_not{int(yrst_train)}-{int(yrst_train+yrnum_train)}_{season}_{zg_str}_LTDManom{BMU_pos_str}.nc\"\n",
    "                SOM_trend_str_cv = f\"/rds/general/project/nowack_graven/live/carl_som_index/data/era5/{zg_str}/crossval/{k}-fold/SOM_cv_{mdl_ens_str}_{n_rows}x{n_columns}_{domain}_{int(yrst_train)}-{int(yrst_train+yrnum_train)}_{season}_{zg_str}_LTDManom{BMU_pos_str}.nc\"\n",
    "                SOM_data_occ_str_cv = f\"/rds/general/project/nowack_graven/live/carl_som_index/data/era5/{zg_str}/crossval/{k}-fold/SOM_data_occur_cv_{mdl_ens_str}_{n_rows}x{n_columns}_{domain}_{int(yrst_train)}-{int(yrst_train+yrnum_train)}_{season}_{zg_str}_LTDManom{BMU_pos_str}.nc\"\n",
    "            else:\n",
    "                print(\"not UKESM or era5 - cannot define save directories\")\n",
    "                return\n",
    "            \n",
    "            JJA_days = int(len(np.unique(file_zg_tot['time.dayofyear'])))\n",
    "            if \"era5\" in file_zg_str:\n",
    "                JJA_days = JJA_days-1 #where there are leap years, need to remove from JJA_days since dayofyear will reflec the different leap year \n",
    "            \n",
    "            #load the training dataset\n",
    "            da_xr_trend_train, all_occur_train, som_yr_train = save_SOM_data(file_zg_tot_train, yrst, yrend, \n",
    "                        season, region, init, neigh, std, ep, rad_0, rad_N, rad_cooling, scale_0, scale_N, scale_cooling,\n",
    "                        savefig_SOM, savefig_trends, n_rows, n_columns, lats_arr, lons_arr, samp, g, zg_str, colormap_str, suptitle, mult_test_method, \n",
    "                            data_yr_reshaped_train, grid_res, vmin, vmax, step, lat_str, lon_str, units, mdl_ens_str, train_input = False, JJA_days=JJA_days, \n",
    "                                                                       BMU_pos=BMU_pos, BMU_pos_thresh=BMU_pos_thresh, save_SOM_str=savefig_SOM)\n",
    "\n",
    "            #print(\"use trained SOM to calculate cv dataset\")\n",
    "            #using the trained SOM, calculate the \n",
    "            da_xr_trend_cv, all_occur_cv, som_yr_train = save_SOM_data(file_zg_tot_cv, yrst, yrend, \n",
    "                        season, region, init, neigh, std, ep, rad_0, rad_N, rad_cooling, scale_0, scale_N, scale_cooling,\n",
    "                        savefig_SOM, savefig_trends, n_rows, n_columns, lats_arr, lons_arr, samp, g, zg_str, colormap_str, suptitle, mult_test_method, \n",
    "                            data_yr_reshaped_cv, grid_res, vmin, vmax, step, lat_str, lon_str, units, mdl_ens_str, train_input = True, som_yr = som_yr_train, JJA_days=JJA_days,\n",
    "                                                                       BMU_pos=BMU_pos, BMU_pos_thresh=BMU_pos_thresh)\n",
    "\n",
    "            print(f\"saving training in {SOM_trend_str_train}\")\n",
    "            da_xr_trend_train.to_netcdf(SOM_trend_str_train)\n",
    "            da_train=xarray.DataArray(all_occur_train, name = \"SOM_data\", dims = (\"row\", \"col\", \"time\"))\n",
    "            da_train['time']=file_zg_tot_train['time']\n",
    "            da_train.to_netcdf(SOM_data_occ_str_train)\n",
    "            print(f\"saving cv in {SOM_trend_str_cv}\")\n",
    "            da_xr_trend_cv.to_netcdf(SOM_trend_str_cv)\n",
    "            da_cv=xarray.DataArray(all_occur_cv, name = \"SOM_data\", dims = (\"row\", \"col\", \"time\"))\n",
    "            da_cv['time']=file_zg_tot_cv['time']\n",
    "            da_cv.to_netcdf(SOM_data_occ_str_cv)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Identify_BMU_from_codebook_posanom(codebook_da_reshaped, data_yr_reshaped, rownum, colnum, BMU_pos_thresh):\n",
    "    \"\"\"\n",
    "    Calculate a new BMU dataset but only defining it by using the anomalies above a specified threshold\n",
    "    \n",
    "    \"\"\"\n",
    "    codebook_da_reshaped_BMU_pos = (codebook_da_reshaped>BMU_pos_thresh)+1-1\n",
    "    bmu_arr=[]\n",
    "    for i, data_day in enumerate(data_yr_reshaped):\n",
    "            if i %1500 == 0:\n",
    "                print(f\"i, data_day = {i}, {data_day}\")\n",
    "            min_euclidean_distance = 1e12\n",
    "            for rowcolnum, (codebook, codebook_pos) in enumerate(zip(codebook_da_reshaped, codebook_da_reshaped_BMU_pos)):\n",
    "                #calculate Euclidean distance between codebook and day in dataset\n",
    "                #but need to exclude the days where there is an anomaly below BMU_pos_thresh\n",
    "                #adjustment here sets the excluded values to zero\n",
    "                codebook_adj = codebook*codebook_pos\n",
    "                data_day_adj = data_day*codebook_pos\n",
    "                euclidean_distance = (np.sum((np.array(data_day.values)-np.array(codebook.values))**2)**0.5)\n",
    "                if euclidean_distance < min_euclidean_distance:\n",
    "                    min_euclidean_distance = euclidean_distance\n",
    "                    min_rowcolnum = rowcolnum\n",
    "            if colnum > rownum:\n",
    "                print(f\"ERROR: colnum {colnum} > rownum {rownum}\")\n",
    "                return 1\n",
    "            bmu_arr.append([min_rowcolnum%colnum, min_rowcolnum//(rownum-(rownum-colnum))])\n",
    "    return bmu_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nrow, ncol = (6, 5)\n",
      "yrnum_train = 4\n",
      "train period 1979-1983\n",
      "save_SOM data_yr_reshaped.shape = (3600, 2346)\n",
      "training, BMU_pos\n",
      "Warning: data was not float32. A 32-bit copy was made\n",
      "i, data_day = 0, <xarray.DataArray 'era5_z_reshaped' (lat_lon: 2346)>\n",
      "array([-46.12978 , -45.953999, -45.381733, ..., 183.248638, 179.317485,\n",
      "       175.184673])\n",
      "Coordinates:\n",
      "    time     datetime64[ns] 1984-05-28T10:30:00\n",
      "Dimensions without coordinates: lat_lon\n",
      "i, data_day = 1500, <xarray.DataArray 'era5_z_reshaped' (lat_lon: 2346)>\n",
      "array([-5.177263,  0.835433,  5.083479, ..., 99.458968, 99.714339, 99.918929])\n",
      "Coordinates:\n",
      "    time     datetime64[ns] 1999-05-28T10:30:00\n",
      "Dimensions without coordinates: lat_lon\n",
      "i, data_day = 3000, <xarray.DataArray 'era5_z_reshaped' (lat_lon: 2346)>\n",
      "array([-17.175429, -20.661269, -23.732069, ..., 212.059434, 210.467638,\n",
      "       208.706407])\n",
      "Coordinates:\n",
      "    time     datetime64[ns] 2014-05-28T10:30:00\n",
      "Dimensions without coordinates: lat_lon\n",
      "save_SOM_str = /rds/general/project/nowack_graven/live/carl_som_index/data/era5/z/crossval/10-fold/plots/SOM_fig_era5_reanal_6x5_EUR_not1979-1983_JJA_extd_z_LTDManom_BMU+.png\n",
      "save_SOM data_yr_reshaped.shape = (500, 2346)\n",
      "i, data_day = 0, <xarray.DataArray 'era5_z_reshaped' (lat_lon: 2346)>\n",
      "array([-87.056987, -79.659526, -70.089701, ...,  14.16567 ,  20.198384,\n",
      "        25.89565 ])\n",
      "Coordinates:\n",
      "    time     datetime64[ns] 1979-05-28T10:30:00\n",
      "Dimensions without coordinates: lat_lon\n",
      "saving training in /rds/general/project/nowack_graven/live/carl_som_index/data/era5/z/crossval/10-fold/SOM_train_era5_reanal_6x5_EUR_not1979-1983_JJA_extd_z_LTDManom_BMU+.nc\n",
      "saving cv in /rds/general/project/nowack_graven/live/carl_som_index/data/era5/z/crossval/10-fold/SOM_cv_era5_reanal_6x5_EUR_1979-1983_JJA_extd_z_LTDManom_BMU+.nc\n",
      "train period 1983-1987\n",
      "save_SOM data_yr_reshaped.shape = (3600, 2346)\n",
      "training, BMU_pos\n",
      "Warning: data was not float32. A 32-bit copy was made\n",
      "i, data_day = 0, <xarray.DataArray 'era5_z_reshaped' (lat_lon: 2346)>\n",
      "array([-87.056987, -79.659526, -70.089701, ...,  14.16567 ,  20.198384,\n",
      "        25.89565 ])\n",
      "Coordinates:\n",
      "    time     datetime64[ns] 1979-05-28T10:30:00\n",
      "Dimensions without coordinates: lat_lon\n",
      "i, data_day = 1500, <xarray.DataArray 'era5_z_reshaped' (lat_lon: 2346)>\n",
      "array([-5.177263,  0.835433,  5.083479, ..., 99.458968, 99.714339, 99.918929])\n",
      "Coordinates:\n",
      "    time     datetime64[ns] 1999-05-28T10:30:00\n",
      "Dimensions without coordinates: lat_lon\n",
      "i, data_day = 3000, <xarray.DataArray 'era5_z_reshaped' (lat_lon: 2346)>\n",
      "array([-17.175429, -20.661269, -23.732069, ..., 212.059434, 210.467638,\n",
      "       208.706407])\n",
      "Coordinates:\n",
      "    time     datetime64[ns] 2014-05-28T10:30:00\n",
      "Dimensions without coordinates: lat_lon\n",
      "save_SOM_str = /rds/general/project/nowack_graven/live/carl_som_index/data/era5/z/crossval/10-fold/plots/SOM_fig_era5_reanal_6x5_EUR_not1983-1987_JJA_extd_z_LTDManom_BMU+.png\n",
      "save_SOM data_yr_reshaped.shape = (500, 2346)\n",
      "i, data_day = 0, <xarray.DataArray 'era5_z_reshaped' (lat_lon: 2346)>\n",
      "array([-49.77694 , -51.855065, -54.164636, ..., 128.164466, 132.297767,\n",
      "       136.694251])\n",
      "Coordinates:\n",
      "    time     datetime64[ns] 1983-05-28T10:30:00\n",
      "Dimensions without coordinates: lat_lon\n",
      "saving training in /rds/general/project/nowack_graven/live/carl_som_index/data/era5/z/crossval/10-fold/SOM_train_era5_reanal_6x5_EUR_not1983-1987_JJA_extd_z_LTDManom_BMU+.nc\n",
      "saving cv in /rds/general/project/nowack_graven/live/carl_som_index/data/era5/z/crossval/10-fold/SOM_cv_era5_reanal_6x5_EUR_1983-1987_JJA_extd_z_LTDManom_BMU+.nc\n",
      "train period 1987-1991\n",
      "save_SOM data_yr_reshaped.shape = (3600, 2346)\n",
      "training, BMU_pos\n",
      "Warning: data was not float32. A 32-bit copy was made\n",
      "i, data_day = 0, <xarray.DataArray 'era5_z_reshaped' (lat_lon: 2346)>\n",
      "array([-87.056987, -79.659526, -70.089701, ...,  14.16567 ,  20.198384,\n",
      "        25.89565 ])\n",
      "Coordinates:\n",
      "    time     datetime64[ns] 1979-05-28T10:30:00\n",
      "Dimensions without coordinates: lat_lon\n",
      "i, data_day = 1500, <xarray.DataArray 'era5_z_reshaped' (lat_lon: 2346)>\n",
      "array([-5.177263,  0.835433,  5.083479, ..., 99.458968, 99.714339, 99.918929])\n",
      "Coordinates:\n",
      "    time     datetime64[ns] 1999-05-28T10:30:00\n",
      "Dimensions without coordinates: lat_lon\n",
      "i, data_day = 3000, <xarray.DataArray 'era5_z_reshaped' (lat_lon: 2346)>\n",
      "array([-17.175429, -20.661269, -23.732069, ..., 212.059434, 210.467638,\n",
      "       208.706407])\n",
      "Coordinates:\n",
      "    time     datetime64[ns] 2014-05-28T10:30:00\n",
      "Dimensions without coordinates: lat_lon\n",
      "save_SOM_str = /rds/general/project/nowack_graven/live/carl_som_index/data/era5/z/crossval/10-fold/plots/SOM_fig_era5_reanal_6x5_EUR_not1987-1991_JJA_extd_z_LTDManom_BMU+.png\n",
      "save_SOM data_yr_reshaped.shape = (500, 2346)\n",
      "i, data_day = 0, <xarray.DataArray 'era5_z_reshaped' (lat_lon: 2346)>\n",
      "array([-21.549628, -25.447089, -29.197578, ..., 249.507012, 244.157891,\n",
      "       238.617364])\n",
      "Coordinates:\n",
      "    time     datetime64[ns] 1987-05-28T10:30:00\n",
      "Dimensions without coordinates: lat_lon\n",
      "saving training in /rds/general/project/nowack_graven/live/carl_som_index/data/era5/z/crossval/10-fold/SOM_train_era5_reanal_6x5_EUR_not1987-1991_JJA_extd_z_LTDManom_BMU+.nc\n",
      "saving cv in /rds/general/project/nowack_graven/live/carl_som_index/data/era5/z/crossval/10-fold/SOM_cv_era5_reanal_6x5_EUR_1987-1991_JJA_extd_z_LTDManom_BMU+.nc\n",
      "train period 1991-1995\n",
      "save_SOM data_yr_reshaped.shape = (3600, 2346)\n",
      "training, BMU_pos\n",
      "Warning: data was not float32. A 32-bit copy was made\n",
      "i, data_day = 0, <xarray.DataArray 'era5_z_reshaped' (lat_lon: 2346)>\n",
      "array([-87.056987, -79.659526, -70.089701, ...,  14.16567 ,  20.198384,\n",
      "        25.89565 ])\n",
      "Coordinates:\n",
      "    time     datetime64[ns] 1979-05-28T10:30:00\n",
      "Dimensions without coordinates: lat_lon\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data_dir = \"/rds/general/user/cmt3718/ephemeral/cmip6/UKESM1-0-LL/piControl/r1i1p1f2/zg/\"\n",
    "LTDM = f\"{data_dir}500zg_day_UKESM1-0-LL_piControl_r1i1p1f2_gn_19600101-20601230_EUR_ydayavg_LTDManom.nc\"\n",
    "LTDM_anom = f\"{data_dir}500zg_day_UKESM1-0-LL_piControl_r1i1p1f2_gn_19600101-20601230_EUR_ydayavg_LTDManom_anom.nc\"\n",
    "anom = f\"{data_dir}500zg_day_UKESM1-0-LL_piControl_r1i1p1f2_gn_19600101-20601230_EUR_anom.nc\"\n",
    "file_zg_str = LTDM_anom\n",
    "mip = \"cmip6\"\n",
    "n_rowcol_arr = range(1,7)\n",
    "#extra row/col combinations\n",
    "n_row_arr = range(7,16)\n",
    "n_col_arr = np.ones((len(n_row_arr)))\n",
    "zg_str = \"zg\"\n",
    "domain = \"EUR\"\n",
    "\n",
    "#number of years in training dataset\n",
    "yrnum_train = 4  \n",
    "\n",
    "\n",
    "file_zg_str = \"/rds/general/project/carl_phd/live/carl/data/era5/day/zg/LTDM/z_timedtrnd_ERA5_1979-2019_EUR_JJAextd_LTDMdaymean_anom_sort.nc\"\n",
    "mip = \"era5\"\n",
    "\n",
    "for nrow, ncol in zip(nrow_vals, ncol_vals):\n",
    "    print(f\"nrow, ncol = {nrow, ncol}\")\n",
    "    for yrnum_train in [4]:\n",
    "        print(f\"yrnum_train = {yrnum_train}\")\n",
    "        SOM_calc_samp(file_zg_str, nrow, ncol, yrnum_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:odin]",
   "language": "python",
   "name": "conda-env-odin-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
